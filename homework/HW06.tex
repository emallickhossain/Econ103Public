\documentclass[addpoints,12pt]{exam}
\usepackage{amsmath, amssymb}
\linespread{1.1}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{multirow}

%To include the answers use:
%	pdflatex "\def\showanswers{1} \input{thisfile.tex}"
\ifdefined\showanswers
  \printanswers
\else
  \noprintanswers
\fi

\title{Problem Set \#6}
\author{Econ 103}
\date{}
\begin{document}
\maketitle

% This problem set concerns continuous random variables

\section*{Part I -- Problems from the Textbook}
Chapter 4: 19, 21, 23
(\emph{When necessary, use R rather than the Normal tables in the front of the textbook.})
Chapter 4: 7, 9, 11, 13, 15, 25, 27, 29\\
Chapter 5: 1, 3, 5, 9, 11, 13, 17

% \section*{Part II -- R Tutorial}
% If you haven't already done so, complete ``R Tutorial \#4'' available at:\\
% \url{http://www.ditraglia.com/econ103/Rtutorial4.html}.
% R Tutorial \# 5 will be posted next week.

% \section*{Part II -- R Tutorial}
% Complete ``R Tutorial \#5'' available at:\\
% \url{http://www.ditraglia.com/econ103/Rtutorial5.html}.


\section*{Part II -- Additional Problems}


\begin{questions}


\question Suppose that $X$ is a random variable with the following PDF
	$$f(x)=\left\{\begin{array}{ll}x & 0\leq x\leq1 \\ 2-x  & 1 \leq x \leq 2\\ 0  & \mbox{otherwise}\end{array}\right. $$	
	\begin{parts}
		\part Graph the PDF of $X$.
			\begin{solution}
				It's an isosceles triangle with base from (0,0) to (2,0) and height 1. 
			\end{solution}
		\part Show that $\int_{-\infty}^\infty f(x)\; dx = 1$.
			\begin{solution}
				\begin{eqnarray*}
				\int_{-\infty}^\infty f(x)\; dx &=& \int_0^1 x\; dx + \int_1^2 (2-x)\; dx = \left.\frac{x^2}{2} \right|_0^1 + \left. \left(2x - \frac{x^2}{2}\right) \right|_1^2 \\ \\
					&=& 1/2 +( 4 - 2) - (2 - 1/2) = 1
				\end{eqnarray*}
			\end{solution}
		\part What is $P(0.5 < X<1.5)$?
			\begin{solution}
				\begin{eqnarray*}
					P(0.5 < X<1.5) &=& \int_{0.5}^{1.5} f(x) \; dx = \int_{0.5}^1 x\; dx + \int_1^{1.5} (2-x)\; dx\\\\
						&=& \left.\frac{x^2}{2} \right|_{0.5}^1 + \left. \left(2x - \frac{x^2}{2}\right) \right|_1^{1.5}\\ \\
			& =& (1/2 - 1/8) + (3 - 9/8) - (2  - 1/2) \\
			&=& 3/8 + 15/8 - 2 + 1/2 =  18/8 - 16/8  + 4/8\\ 
			&=& 6/8 = 3/4 = 0.75
				\end{eqnarray*}
			\end{solution}
	\end{parts}
	
	

\question A random variable is said to follow a Uniform$(a,b)$ distribution if it is equally likely to take on any value in the range $[a,b]$ and never takes a value outside this range. Suppose that $X$ is such a random variable, i.e.\ $X \sim \mbox{Uniform}(a,b)$.
	\begin{parts}
		\part What is the support of $X$?
			\begin{solution}
				$[a,b]$
			\end{solution}
		\part Explain why the PDF of $X$ is $f(x) = 1/(b-a)$ for $a\leq x \leq b$, zero elsewhere.
			\begin{solution}
				This simply generalizes the Uniform$(0,1)$ random variable from class. To capture the idea that $X$ is equally likely to take on any value in the range $[a,b]$, the PDF must be constant. To ensure that it integrates to 1, the denominator must be $b-a$. 
			\end{solution}
		\part Using the PDF from part (b), calculate the CDF of $X$.
		\begin{solution}
		 \begin{eqnarray*}
		 	F(x_0) &=&\int_{-\infty}^{x_0} f(x)\; dx = \int_{a}^{x_0} \frac{dx}{b-a} = \left.\frac{x}{b-a}\right|_a^{x_0} =  \frac{x_0-a}{b-a} 
		\end{eqnarray*}
		 \end{solution}
		\part Verify that $f(x) = F'(x)$ for the present example.	
		\begin{solution}
			\begin{eqnarray*}
				F'(x) = \frac{d}{dx} \left(\frac{x - a}{b-a}\right) = \frac{1}{b-a} = f(x)
			\end{eqnarray*}
		\end{solution}
		\part Calculate $E[X]$.
			\begin{solution}
			\begin{eqnarray*}
				E[X] &=& \int_{-\infty}^\infty xf(x)\; dx = \int_a^b \frac{x}{b-a}\; dx = \left.\frac{x^2}{2(b-a)}\right|_a^b =  \frac{b^2 - a^2}{2(b-a)} = \frac{a+b}{2}
			\end{eqnarray*}
			\end{solution}
		\part Calculate $E[X^2]$. \emph{Hint:} recall that $b^3-a^3$ can be factorized as $(b-a)(b^2 + a^2 + ab)$.
			\begin{solution}
				\begin{eqnarray*}
					E[X^2]& =& \int_{-\infty}^\infty x^2 f(x) \; dx = \int_a^b \frac{x^2}{b-a} = \left. \frac{x^3}{3(b-a)}\right|_a^b = \frac{b^3 - a^3}{3(b-a)}\\ \\
					&=& \frac{(b-a)(b^2 + a^2 + ab)}{3(b-a)} = \frac{b^2 + a^2 + ab}{3}
				\end{eqnarray*}
			\end{solution}
		\part Using the shortcut formula and parts (e) and (f),  show that $Var(X) = (b-a)^2/12$.
			\begin{solution}
				\begin{eqnarray*}
					Var(X) &=& E[X^2] - \left(E[X]\right)^2 = \frac{b^2 + a^2 + ab}{3} - \left(\frac{a+b}{2}\right)^2\\\\
					&=& \frac{b^2 + a^2 + ab}{3} -\frac{a^2 + 2ab + b^2}{4}\\\\
					&=& \frac{4b^2 + 4a^2 + 4ab - 3a^2 - 6ab - 3b^2}{12}\\\\
					&=& \frac{b^2 + a^2 - 2ab}{12} = \frac{(b-a)^2}{12}
				\end{eqnarray*}
			\end{solution}
	\end{parts}

	
\question Suppose that $X\sim N(0, 16)$ independent of $Y\sim N(2, 4)$. Recall that our convention is to express the normal distribution in terms of its mean and variance, i.e.\ $N(\mu, \sigma^2)$. Hence, $X$ has a mean of zero and variance of 16, while $Y$ has a mean of 2 and a variance of 4. In completing some parts of this question you will need to use the R function \texttt{pnorm} described in class. In this case, please write down the command you used as well as the numeric result.
	\begin{parts}
		\part Calculate $P( -8\leq X \leq 8)$.
			\begin{solution}
				\begin{eqnarray*}
				P( -8\leq X \leq 8) = P( -8/4\leq X/4 \leq 8/4) = P( -2\leq Z \leq 	2)\approx 0.95
				\end{eqnarray*}
where $Z$ is a standard normal random variable.
			\end{solution}
		\part Calculate $P(0 \leq Y \leq 4)$.
					\begin{solution}
			\begin{eqnarray*}
				P(0 \leq Y \leq 4) = P\left(\frac{0-2}{2} \leq \frac{Y-2}{2}\leq \frac{4-2}{2}\right) = P(-1 \leq Z \leq 1) \approx 0.68
			\end{eqnarray*}
			where $Z$ is a standard normal random variable.
			\end{solution}
		\part Calculate $P( -1\leq Y \leq 6)$.
					\begin{solution}
				\begin{eqnarray*}
					P( -1\leq Y \leq 6) &=& P\left( \frac{-1 -2}{2}\leq \frac{Y - 2}{2} \leq \frac{6-2}{2}\right)\\\\
					&=& P(-1.5 \leq Z \leq 2)\\
					&=& \Phi(2) - \Phi(-1.5)\\
					&=& \mbox{\texttt{pnorm(2) - pnorm(-1.5)}}\\
					&\approx& 0.91
				\end{eqnarray*}
				where $Z$ is a standard normal random variable.
			\end{solution}
		\part Calculate $P(X \geq 10)$.
					\begin{solution}
			\begin{eqnarray*}
				P(X \geq 10) &=& 1 - P(X \leq 10) = 1 - P(X /4\leq 10/4) = 1 - P(Z\leq 2.5) \\
				&=& 1 - \Phi(2.5) = 1 - \mbox{\texttt{pnorm(2.5)}}\\
				&\approx& 0.006
			\end{eqnarray*}
			\end{solution}
	\end{parts}	
	

\noindent \paragraph{Note:} \emph{In the following five questions  $X_1, X_2 \sim \mbox{ iid } N(\mu, \sigma^2)$, $Y = (X_1 - \mu)/\sigma$, $Z = (X_2 - \mu)/\sigma$.}


\question 
	\begin{parts}
		\part What is the distribution of $X_1 + X_2$?
			\begin{solution}
				$X_1 + X_2 \sim N(2\mu, 2\sigma^2)$
			\end{solution}
		\part Use R to calculate $P(X_1 + X_2 > 5)$ if $\mu = 5$ and $\sigma^2 = 50$.
			\begin{solution}
			In this case, $X_1 + X_2 \sim N(10, 100)$, hence
			\begin{eqnarray*}
				P(X_1 + X_2 > 5) &=& 1 - P(X_1 + X_2 \leq 5) \\
				&=& 1 - P\left(\frac{X_1 + X_2 - 10}{10} \leq \frac{5-10}{10}\right)\\
				&=& 1 - \mbox{\texttt{pnorm(-0.5)}}\\
				&\approx& 0.6914625
			\end{eqnarray*}
			Alternatively, we could use  \texttt{1- pnorm(5, mean = 10, sd = 10)}, which gives the same result.
			\end{solution}
		\part Use R to calculate the 10th percentile of the distribution of $X_1 + X_2$.
			\begin{solution}
				\texttt{qnorm(p = 0.1, mean = 10, sd = 10)} gives -2.815516.
			\end{solution}
	\end{parts}
\question 
	\begin{parts}
		\part What is the distribution of $Y^2$?
			\begin{solution}
				As the sum of squares of one standard normal RV, $Y^2 \sim \chi^2(1)$.
			\end{solution}
		\part Use R to calculate $P(Y^2 \geq 1)$.
			\begin{solution}
				\begin{eqnarray*}
					P(Y^2 \geq 1) = 1 - P(Y^2 \leq 1) = 1 -  \mbox{\texttt{pchisq(1, df = 1)}} \approx 0.3173105
				\end{eqnarray*}
			\end{solution}
	\end{parts}
	
\question
	\begin{parts}
		\part What is the distribution of $Y^2 + Z^2$?
			\begin{solution}
			Since this is the sum of squares of two independent standard normal random variables, $Y^2 + Z^2 \sim \chi^2(2)$.
			\end{solution}
		\part Use R to calculate the 95th percentile of the distribution of $Y^2 + Z^2$.
			\begin{solution}
				\texttt{qchisq(p = 0.95, df = 2)} gives 5.991465
			\end{solution}
	\end{parts}
\question 
	\begin{parts}
	\part What is the distribution of $Z/\sqrt{Y^2}$?
		\begin{solution}
			Since it is the ratio of a standard normal to the square root of an independent $\chi^2$ random variable divided by its degrees of freedom (in this case one), $Z/\sqrt{Y^2}\sim t(1)$.
		\end{solution}
	\part What value of $c$ satisfies $P(-c\ \leq Z/\sqrt{Y^2} \leq c )=0.95$?
		\begin{solution}
			By the symmetry of the $t$-distribution, it suffices to find the $97.5$th percentile (this allocates 2.5\% probability to the upper and lower tails).  The command \texttt{qt(p = 0.975, df = 1)} gives 12.7062, so $c \approx 12.7$. Alternatively, we could have calculated the 2.5th percentile: \texttt{qt(p = 0.025, df = 1)} gives -12.7062.
		\end{solution}
	\part How does the interval in part (b) compare to the corresponding interval for $Z$?
		\begin{solution}
		Since $Z$ is a standard normal RV, $P(-2\leq Z \leq 2) \approx 0.95$. We see that the interval for a $t(1)$ RV is \emph{much wider} than the corresponding interval for a standard normal. In other words, extreme outcomes are much more likely under the $t(1)$ distribution.
		\end{solution}
	\end{parts}
	
\question 
	\begin{parts}
		\part What is the distribution of $Y^2/Z^2$?
			\begin{solution}
				This is the ratio of two independent $\chi^2$ random variables, each divided by its degrees of freedom (in this case, one). Hence $Y^2/Z^2\sim F(1,1)$.
			\end{solution}
		\part Use R to calculate the 95th percentile of the distribution of $Y^2/Z^2$.
			\begin{solution}
				\texttt{qf(p = 0.95, df1 = 1, df2 = 1)} gives 161.4476
			\end{solution}
	\end{parts}
	
	\question Fill in the missing details from class to calculate the variance of a Bernoulli Random Variable \emph{directly}, that is \emph{without} using the shortcut formula.
	\begin{solution} 
	\begin{eqnarray*}
	\sigma^2 &=& Var(X) = \sum_{x \in \{0,1\}} (x - \mu)^2 p(x)\\ 
	&=& \sum_{x \in \{0,1\}} (x - p)^2 p(x)\\
	 &=& (0 - p)^2 (1-p) + (1-p)^2 p \\
	 &=& p^2(1-p) + (1-p)^2 p\\ 
	 &=& p^2 - p^3 + p - 2p^2 +p^3 \\
	 &=& p - p^2\\ 
	 &=&p(1-p)
\end{eqnarray*}
	\end{solution}

\question Prove that the Bernoulli Random Variable is a special case of the Binomial Random variable for which $n = 1$.	 (Hint: compare pmfs.)
	\begin{solution}
		The pmf for a Binomial$(n,p)$ random variable is
		$$p(x) = {n \choose x} p^x (1-p)^{n-x}$$
		with support $\{0, 1, 2\hdots, n\}$. Setting $n=1$ gives,
		$$p(x) = p(x) = {1 \choose x} p^x (1-p)^{1-x}$$
		with support $\{0,1\}$. Plugging in each realization in the support, and recalling that $0! = 1$, we have
			$$p(0) = \frac{1!}{0!(1-0)!} p^0 (1-p)^{1-0} = 1 - p$$
		and
		$$p(1) = \frac{1!}{1!(1-1)!} p^1 (1-p)^0 = p$$
		which is exactly how we defined the Bernoulli Random Variable.
	\end{solution}
	
\question Suppose that $X$ is a random variable with support $\{1,2\}$ and $Y$ is a random variable with support $\{0,1\}$ where $X$ and $Y$ have the following joint distribution:
			\begin{eqnarray*}
				p_{XY}(1,0) = 0.20, && p_{XY}(1,1) = 0.30 \\
				p_{XY}(2,0) = 0.25, && p_{XY}(2,1) = 0.25
			\end{eqnarray*}
	\begin{parts}
		\item Express the joint distribution in a $2\times 2$ table.
			\begin{solution}
			\begin{center}
\begin{tabular}{|cc|cc|}
\hline
&&\multicolumn{2}{c|}{$X$}\\
&&1 & 2\\
\hline
\multirow{2}{*}{$Y$}
&0& \multicolumn{1}{|c}{0.20} & 0.25\\
&1& \multicolumn{1}{|c}{0.30} & 0.25\\
\hline
\end{tabular}
\end{center}
			\end{solution}
		\item Using the table, calculate the marginal probability distributions of $X$ and $Y$.
			\begin{solution}
				\begin{eqnarray*}
					p_X(1) &=&p_{XY}(1,0) + p_{XY}(1,1)=0.20+0.30 = 0.50 \\
					p_X(2) &=&p_{XY}(2,0) + p_{XY}(2,1)=0.25 + 0.25 = 0.50 \\
					p_Y(0) &=&p_{XY}(1,0) + p_{XY}(2,0) = 0.20 + 0.25 = 0.45 \\
					p_Y(1) &=& p_{XY}(1,1) + p_{XY}(2,1) = 0.30 + 0.25 = 0.55
				\end{eqnarray*}
			\end{solution}
		\item Calculate the conditional probability distribution of $Y|X=1$ and $Y|X=2$.
			\begin{solution}
			The distribution of $Y|X = 1$ is
				\begin{eqnarray*}
					P(Y = 0|X = 1) &=&\frac{p_{XY}(1,0)}{p_X(1)} = \frac{0.2}{0.5}=0.4\\\\
					P(Y = 1|X= 1) &=&\frac{p_{XY}(1,1)}{p_X(1)} = \frac{0.3}{0.5} = 0.6
				\end{eqnarray*}
				while the distribution of $Y|X = 2$ is
				\begin{eqnarray*}
					P(Y = 0|X = 2) &=&\frac{p_{XY}(2,0)}{p_X(2)} = \frac{0.25}{0.5} = 0.5\\\\
					P(Y = 1|X= 2) &=&\frac{p_{XY}(2,1)}{p_X(2)} = \frac{0.25}{0.5} = 0.5
				\end{eqnarray*}
			\end{solution}
		\item Calculate $E[Y|X]$.
			\begin{solution}
			\begin{eqnarray*}
				E[Y | X =1 ] &=& 0 \times 0.4 + 1 \times 0.6 = 0.6\\
				E[Y | X =2 ] &=& 0 \times 0.5 + 1 \times 0.5 = 0.5
			\end{eqnarray*}
			Hence, 
				$$E[Y|X] = \left\{ \begin{array}{ll} 0.6  & \mbox{with probability } 0.5\\ 0.5& \mbox{with probability } 0.5\end{array} \right.$$
				since $p_X(1) = 0.5$ and $p_X(2) = 0.5$.
			\end{solution}
		\item What is $E[E[Y|X]]$?
			\begin{solution}
			$E[E[Y|X]] = 0.5 \times 0.6 + 0.5 \times 0.5 = 0.3 + 0.25 = 0.55$. Note that this equals the expectation of $Y$ calculated from its marginal distribution, since $E[Y] = 0 \times 0.45 + 1 \times 0.55$. This illustrates the so-called ``Law of Iterated Expectations.''
			\end{solution}
		\item Calculate the covariance between $X$ and $Y$ using the shortcut formula.
		\begin{solution}
		First, from the marginal distributions, $E[X] = 1\cdot 0.5 + 2 \cdot 0.5 = 1.5$ and $E[Y]=0 \cdot 0.45 + 1 \cdot 0.55 = 0.55$. Hence $E[X]E[Y] = 1.5 \cdot 0.55 = 0.825$. Second,
			\begin{eqnarray*}
				E[XY] &=& (0\cdot 1) \cdot 0.2 + (0\cdot 2)\cdot 0.25 + (1\cdot 1) \cdot 0.3 + (1\cdot 2) 0.25\\
						&=& 0.3 + 0.5 = 0.8
			\end{eqnarray*}
			Finally $Cov(X,Y) = E[XY] - E[X]E[Y] = 0.8 - 0.825 = -0.025$
		\end{solution}
	\end{parts}

\question Let $X$ and $Y$ be discrete random variables and $a,b,c,d$ be constants. Prove the following:
	\begin{parts}
		\part $Cov(a+bX, c + dY) = bd Cov(X,Y)$
		\begin{solution}
		Let $\mu_X = E[X]$ and $\mu_Y = E[Y]$. By the linearity of expectation,
			\begin{eqnarray*}
				E[a + bX] &=& a + b\mu_X\\
				E[c + dY] &=& c + d\mu_Y
			\end{eqnarray*}
	Thus, we have
			\begin{eqnarray*}
				(a+bx) - E[a + bX]&=& b(x - \mu_X)\\
				(c + dy) - E[c + dY]&=& d(y-\mu_Y)
			\end{eqnarray*}
	Substituting these into the formula for the covariance between two discrete random variables,
			\begin{eqnarray*}
				Cov(a+bX, c+dY) &=& \sum_{x} \sum_{y} \left[b(x - \mu_X)\right]\left[d(y-\mu_Y)\right]p(x,y)\\
					&=&bd\sum_{x} \sum_{y} (x - \mu_X)(y-\mu_Y)p(x,y)\\
					&=&bd Cov(X,Y)
			\end{eqnarray*}
		\end{solution}
		\part $Corr(a+bX, c + dY) = Corr(X,Y)$
		\begin{solution}
			\begin{eqnarray*}
				Corr(a+bX, c + dY) &=& \frac{Cov(a+bX, c+dY)}{\sqrt{Var(a+bX)Var(c+dY)}}\\
				&=& \frac{bd Cov(X,Y)}{\sqrt{b^2Var(X)d^2Var(Y)}}\\
				&=&\frac{ Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\\
				&=& Corr(X,Y)
			\end{eqnarray*}
		\end{solution}
	\end{parts}


\question Fill in the missing steps from lecture to prove the shortcut formula for covariance:
	$$Cov(X,Y) = E[XY] - E[X]E[Y]$$
	\begin{solution}
	By the Linearity of Expectation,
	\begin{eqnarray*}
	Cov(X,Y)&=& E[(X - \mu_X)(Y-\mu_Y)]\\
			&=& E[XY - \mu_X Y - \mu_Y X + \mu_X \mu_Y]\\
			&=&E[XY] - \mu_xE[Y] - \mu_Y E[X] + \mu_X \mu_Y\\
			&=& E[XY] - \mu_X\mu_Y - \mu_Y\mu_X + \mu_X \mu_Y\\
			&=& E[XY] - \mu_X \mu_Y\\
			&=& E[XY] - E[X]E[Y]
\end{eqnarray*}
	\end{solution}


\question Let $X_1$ be a random variable denoting the returns of stock 1, and $X_2$ be a random variable denoting the returns of stock 2. Accordingly let $\mu_1 = E[X_1]$, $\mu_2 = E[X_2]$, $\sigma_1^2 = Var(X_1)$, $\sigma_2^2 = Var(X_2)$ and $\rho = Corr(X_1, X_2)$. A \emph{portfolio}, $\Pi$,  is a linear combination of $X_1$ and $X_2$ with weights that sum to one, that is $\Pi(\omega) = \omega X_1 + (1-\omega)X_2$, indicating the proportions of stock 1 and stock 2 that an investor holds. In this example, we require $\omega \in [0,1]$, so that \emph{negative} weights are not allowed. (This rules out short-selling.) 
	\begin{parts}
		\part Calculate $E[\Pi(\omega)]$ in terms of $\omega$, $\mu_1$ and $\mu_2$.
			\begin{solution}
				\begin{eqnarray*}
					E[\Pi(\omega)] &=& E[\omega X_1 + (1-\omega) X_2] = \omega E[X_1] + (1-\omega) E[X_2]\\
						&=& \omega \mu_1 + (1-\omega) \mu_2
				\end{eqnarray*}
			\end{solution}
		\part If $\omega \in [0,1]$ is it possible to have $E[\Pi(\omega)]>\mu_1$ \emph{and} $E[\Pi(\omega)]>\mu_2$? What about $E[\Pi(\omega)]<\mu_1$ and $E[\Pi(\omega)]<\mu_2$? Explain.
			\begin{solution}
			No. If short-selling is disallowed, the portfolio expected return must be between $\mu_1$ and $\mu_2$.
			\end{solution}
		\part Express $Cov(X_1, X_2)$ in terms of $\rho$ and $\sigma_1$, $\sigma_2$.
			\begin{solution}
				$Cov(X,Y) = \rho \sigma_1 \sigma_2$
			\end{solution}
		\part What is $Var[\Pi(\omega)]$? (Your answer should be in terms of $\rho$, $\sigma_1^2$ and $\sigma_2^2$.)
			\begin{solution}
				\begin{eqnarray*}
					Var[\Pi(\omega)] &=& Var[\omega X_1 + (1-\omega) X_2]\\
					& =& \omega^2 Var(X_1) + (1-\omega)^2 Var(X_2) + 2\omega (1-\omega) Cov(X_1, X_2)\\
					&=&\omega^2 \sigma_1^2 + (1-\omega)^2\sigma_2^2 + 2\omega (1-\omega)\rho \sigma_1 \sigma_2 
				\end{eqnarray*}
			\end{solution}
		\part Using part (d) show that the value of $\omega$ that minimizes $Var[\Pi(\omega)]$ is
			$$\omega^* = \frac{\sigma_2^2 - \rho\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 -2\rho\sigma_1\sigma_2}$$
			In other words, $\Pi(\omega^*)$ is the \emph{minimum variance portfolio}. 
		\begin{solution}
			The First Order Condition is:
				$$2\omega \sigma_1^2 - 2(1-\omega)\sigma_2^2 + (2 - 4\omega)\rho \sigma_1\sigma_2=0$$
				Dividing both sides by two and rearranging:
				\begin{eqnarray*}
				\omega \sigma_1^2 - (1-\omega)\sigma_2^2 + (1-2\omega)\rho\sigma_1 \sigma_2&=&0\\
				\omega\sigma_1^2 - \sigma_2^2 + \omega \sigma_2^2 + \rho \sigma_1\sigma_2 - 2\omega \rho \sigma_1\sigma_2 &=& 0\\
				\omega(\sigma_1^2 + \sigma_2^2 -2\rho\sigma_1\sigma_2) &=& \sigma_2^2 - \rho\sigma_1\sigma_2
				\end{eqnarray*}
				So we have
					$$\omega^* = \frac{\sigma_2^2 - \rho\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 -2\rho\sigma_1\sigma_2}$$
		\end{solution}
		\part If you want a challenge, check the second order condition from part (e).
			\begin{solution}
							The second derivative is
				$$2\sigma_1^2 - 2\sigma_2^2 - 4\rho \sigma_1\sigma_2$$
				and, since $\rho=1$ is the largest possible value for $\rho$,
				$$2\sigma_1^2 - 2\sigma_2^2 - 4\rho \sigma_1\sigma_2 \geq 2\sigma_1^2 - 2\sigma_2^2 - 4\sigma_1\sigma_2 = 2(\sigma_1 - \sigma_2)^2 \geq 0$$
				so the second derivative is positive, indicating a minimum. This is a global minimum since the problem is quadratic in $\omega$.
			\end{solution}
	\end{parts}


\end{questions}


\end{document}
