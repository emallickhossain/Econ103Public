\documentclass[addpoints,12pt]{exam}
\usepackage{amsmath, amssymb}
\linespread{1.1}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{enumitem}

%\printanswers


\title{Problem Set \#5}
\author{Econ 103}
\date{}
\begin{document}
\maketitle

\section*{Lecture Progress}
We made it to the end of the Chapter 4 (Updated) slides.

\section*{Homework Checklist}

\begin{itemize}[label = $\square$]
	\item \textbf{Book Problems (Chapter 4):} 7, 11, 15, 25, 27, 29
	\item \textbf{Book Problems (Chapter 5):} 1, 3, 5, 9, 11, 13, 17
	\item \textbf{Additional Problems: }See below
	\item\textbf{R Tutorial:} R Tutorial 4
	\item \textbf{Ask questions on Piazza}
	\item\textbf{Review slides}
\end{itemize}

\section*{Part II -- Additional Problems}
\begin{questions}


	\question Fill in the missing details from class to calculate the variance of a Bernoulli Random Variable \emph{directly}, that is \emph{without} using the shortcut formula.
	\begin{solution} 
	\begin{eqnarray*}
	\sigma^2 &=& Var(X) = \sum_{x \in \{0,1\}} (x - \mu)^2 p(x)\\ 
	&=& \sum_{x \in \{0,1\}} (x - p)^2 p(x)\\
	 &=& (0 - p)^2 (1-p) + (1-p)^2 p \\
	 &=& p^2(1-p) + (1-p)^2 p\\ 
	 &=& p^2 - p^3 + p - 2p^2 +p^3 \\
	 &=& p - p^2\\ 
	 &=&p(1-p)
\end{eqnarray*}
	\end{solution}

\question Prove that the Bernoulli Random Variable is a special case of the Binomial Random variable for which $n = 1$.	 (Hint: compare pmfs.)
	\begin{solution}
		The pmf for a Binomial$(n,p)$ random variable is
		$$p(x) = {\binom{n}{x}} p^x (1-p)^{n-x}$$
		with support $\{0, 1, 2\hdots, n\}$. Setting $n=1$ gives,
		$$p(x) = p(x) = {\binom{1}{x}} p^x (1-p)^{1-x}$$
		with support $\{0,1\}$. Plugging in each realization in the support, and recalling that $0! = 1$, we have
			$$p(0) = \frac{1!}{0!(1-0)!} p^0 (1-p)^{1-0} = 1 - p$$
		and
		$$p(1) = \frac{1!}{1!(1-1)!} p^1 (1-p)^0 = p$$
		which is exactly how we defined the Bernoulli Random Variable.
	\end{solution}
	
\question Suppose that $X$ is a random variable with support $\{1,2\}$ and $Y$ is a random variable with support $\{0,1\}$ where $X$ and $Y$ have the following joint distribution:
			\begin{eqnarray*}
				p_{XY}(1,0) = 0.20, && p_{XY}(1,1) = 0.30 \\
				p_{XY}(2,0) = 0.25, && p_{XY}(2,1) = 0.25
			\end{eqnarray*}
	\begin{parts}
		\item Express the joint distribution in a $2\times 2$ table.
			\begin{solution}
			\begin{center}
\begin{tabular}{|cc|cc|}
\hline
&&\multicolumn{2}{c|}{$X$}\\
&&1 & 2\\
\hline
\multirow{2}{*}{$Y$}
&0& \multicolumn{1}{|c}{0.20} & 0.25\\
&1& \multicolumn{1}{|c}{0.30} & 0.25\\
\hline
\end{tabular}
\end{center}
			\end{solution}
		\item Using the table, calculate the marginal probability distributions of $X$ and $Y$.
			\begin{solution}
				\begin{eqnarray*}
					p_X(1) &=&p_{XY}(1,0) + p_{XY}(1,1)=0.20+0.30 = 0.50 \\
					p_X(2) &=&p_{XY}(2,0) + p_{XY}(2,1)=0.25 + 0.25 = 0.50 \\
					p_Y(0) &=&p_{XY}(1,0) + p_{XY}(2,0) = 0.20 + 0.25 = 0.45 \\
					p_Y(1) &=& p_{XY}(1,1) + p_{XY}(2,1) = 0.30 + 0.25 = 0.55
				\end{eqnarray*}
			\end{solution}
		\item Calculate the conditional probability distribution of $Y|X=1$ and $Y|X=2$.
			\begin{solution}
			The distribution of $Y|X = 1$ is
				\begin{eqnarray*}
					P(Y = 0|X = 1) &=&\frac{p_{XY}(1,0)}{p_X(1)} = \frac{0.2}{0.5}=0.4\\\\
					P(Y = 1|X= 1) &=&\frac{p_{XY}(1,1)}{p_X(1)} = \frac{0.3}{0.5} = 0.6
				\end{eqnarray*}
				while the distribution of $Y|X = 2$ is
				\begin{eqnarray*}
					P(Y = 0|X = 2) &=&\frac{p_{XY}(2,0)}{p_X(2)} = \frac{0.25}{0.5} = 0.5\\\\
					P(Y = 1|X= 2) &=&\frac{p_{XY}(2,1)}{p_X(2)} = \frac{0.25}{0.5} = 0.5
				\end{eqnarray*}
			\end{solution}
		\item Calculate $E[Y|X]$.
			\begin{solution}
			\begin{eqnarray*}
				E[Y | X =1 ] &=& 0 \times 0.4 + 1 \times 0.6 = 0.6\\
				E[Y | X =2 ] &=& 0 \times 0.5 + 1 \times 0.5 = 0.5
			\end{eqnarray*}
			Hence, 
				$$E[Y|X] = \left\{ \begin{array}{ll} 0.6  & \mbox{with probability } 0.5\\ 0.5& \mbox{with probability } 0.5\end{array} \right.$$
				since $p_X(1) = 0.5$ and $p_X(2) = 0.5$.
			\end{solution}
		\item What is $E[E[Y|X]]$?
			\begin{solution}
			$E[E[Y|X]] = 0.5 \times 0.6 + 0.5 \times 0.5 = 0.3 + 0.25 = 0.55$. Note that this equals the expectation of $Y$ calculated from its marginal distribution, since $E[Y] = 0 \times 0.45 + 1 \times 0.55$. This illustrates the so-called ``Law of Iterated Expectations.''
			\end{solution}
		\item Calculate the covariance between $X$ and $Y$ using the shortcut formula.
		\begin{solution}
		First, from the marginal distributions, $E[X] = 1\cdot 0.5 + 2 \cdot 0.5 = 1.5$ and $E[Y]=0 \cdot 0.45 + 1 \cdot 0.55 = 0.55$. Hence $E[X]E[Y] = 1.5 \cdot 0.55 = 0.825$. Second,
			\begin{eqnarray*}
				E[XY] &=& (0\cdot 1) \cdot 0.2 + (0\cdot 2)\cdot 0.25 + (1\cdot 1) \cdot 0.3 + (1\cdot 2) 0.25\\
						&=& 0.3 + 0.5 = 0.8
			\end{eqnarray*}
			Finally $Cov(X,Y) = E[XY] - E[X]E[Y] = 0.8 - 0.825 = -0.025$
		\end{solution}
	\end{parts}

\question Let $X$ and $Y$ be discrete random variables and $a,b,c,d$ be constants. Prove the following:
	\begin{parts}
		\part $Cov(a+bX, c + dY) = bd Cov(X,Y)$
		\begin{solution}
		Let $\mu_X = E[X]$ and $\mu_Y = E[Y]$. By the linearity of expectation,
			\begin{eqnarray*}
				E[a + bX] &=& a + b\mu_X\\
				E[c + dY] &=& c + d\mu_Y
			\end{eqnarray*}
	Thus, we have
			\begin{eqnarray*}
				(a+bx) - E[a + bX]&=& b(x - \mu_X)\\
				(c + dy) - E[c + dY]&=& d(y-\mu_Y)
			\end{eqnarray*}
	Substituting these into the formula for the covariance between two discrete random variables,
			\begin{eqnarray*}
				Cov(a+bX, c+dY) &=& \sum_{x} \sum_{y} \left[b(x - \mu_X)\right]\left[d(y-\mu_Y)\right]p(x,y)\\
					&=&bd\sum_{x} \sum_{y} (x - \mu_X)(y-\mu_Y)p(x,y)\\
					&=&bd Cov(X,Y)
			\end{eqnarray*}
		\end{solution}
		\part $Corr(a+bX, c + dY) = Corr(X,Y)$
		\begin{solution}
			\begin{eqnarray*}
				Corr(a+bX, c + dY) &=& \frac{Cov(a+bX, c+dY)}{\sqrt{Var(a+bX)Var(c+dY)}}\\
				&=& \frac{bd Cov(X,Y)}{\sqrt{b^2Var(X)d^2Var(Y)}}\\
				&=&\frac{ Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\\
				&=& Corr(X,Y)
			\end{eqnarray*}
		\end{solution}
	\end{parts}


\question Fill in the missing steps from lecture to prove the shortcut formula for covariance:
	$$Cov(X,Y) = E[XY] - E[X]E[Y]$$
	\begin{solution}
	By the Linearity of Expectation,
	\begin{eqnarray*}
	Cov(X,Y)&=& E[(X - \mu_X)(Y-\mu_Y)]\\
			&=& E[XY - \mu_X Y - \mu_Y X + \mu_X \mu_Y]\\
			&=&E[XY] - \mu_xE[Y] - \mu_Y E[X] + \mu_X \mu_Y\\
			&=& E[XY] - \mu_X\mu_Y - \mu_Y\mu_X + \mu_X \mu_Y\\
			&=& E[XY] - \mu_X \mu_Y\\
			&=& E[XY] - E[X]E[Y]
\end{eqnarray*}
	\end{solution}


\question Let $X_1$ be a random variable denoting the returns of stock 1, and $X_2$ be a random variable denoting the returns of stock 2. Accordingly let $\mu_1 = E[X_1]$, $\mu_2 = E[X_2]$, $\sigma_1^2 = Var(X_1)$, $\sigma_2^2 = Var(X_2)$ and $\rho = Corr(X_1, X_2)$. A \emph{portfolio}, $\Pi$,  is a linear combination of $X_1$ and $X_2$ with weights that sum to one, that is $\Pi(\omega) = \omega X_1 + (1-\omega)X_2$, indicating the proportions of stock 1 and stock 2 that an investor holds. In this example, we require $\omega \in [0,1]$, so that \emph{negative} weights are not allowed. (This rules out short-selling.) 
	\begin{parts}
		\part Calculate $E[\Pi(\omega)]$ in terms of $\omega$, $\mu_1$ and $\mu_2$.
			\begin{solution}
				\begin{eqnarray*}
					E[\Pi(\omega)] &=& E[\omega X_1 + (1-\omega) X_2] = \omega E[X_1] + (1-\omega) E[X_2]\\
						&=& \omega \mu_1 + (1-\omega) \mu_2
				\end{eqnarray*}
			\end{solution}
		\part If $\omega \in [0,1]$ is it possible to have $E[\Pi(\omega)]>\mu_1$ \emph{and} $E[\Pi(\omega)]>\mu_2$? What about $E[\Pi(\omega)]<\mu_1$ and $E[\Pi(\omega)]<\mu_2$? Explain.
			\begin{solution}
			No. If short-selling is disallowed, the portfolio expected return must be between $\mu_1$ and $\mu_2$.
			\end{solution}
		\part Express $Cov(X_1, X_2)$ in terms of $\rho$ and $\sigma_1$, $\sigma_2$.
			\begin{solution}
				$Cov(X,Y) = \rho \sigma_1 \sigma_2$
			\end{solution}
		\part What is $Var[\Pi(\omega)]$? (Your answer should be in terms of $\rho$, $\sigma_1^2$ and $\sigma_2^2$.)
			\begin{solution}
				\begin{eqnarray*}
					Var[\Pi(\omega)] &=& Var[\omega X_1 + (1-\omega) X_2]\\
					& =& \omega^2 Var(X_1) + (1-\omega)^2 Var(X_2) + 2\omega (1-\omega) Cov(X_1, X_2)\\
					&=&\omega^2 \sigma_1^2 + (1-\omega)^2\sigma_2^2 + 2\omega (1-\omega)\rho \sigma_1 \sigma_2 
				\end{eqnarray*}
			\end{solution}
		\part Using part (d) show that the value of $\omega$ that minimizes $Var[\Pi(\omega)]$ is
			$$\omega^* = \frac{\sigma_2^2 - \rho\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 -2\rho\sigma_1\sigma_2}$$
			In other words, $\Pi(\omega^*)$ is the \emph{minimum variance portfolio}. 
		\begin{solution}
			The First Order Condition is:
				$$2\omega \sigma_1^2 - 2(1-\omega)\sigma_2^2 + (2 - 4\omega)\rho \sigma_1\sigma_2=0$$
				Dividing both sides by two and rearranging:
				\begin{eqnarray*}
				\omega \sigma_1^2 - (1-\omega)\sigma_2^2 + (1-2\omega)\rho\sigma_1 \sigma_2&=&0\\
				\omega\sigma_1^2 - \sigma_2^2 + \omega \sigma_2^2 + \rho \sigma_1\sigma_2 - 2\omega \rho \sigma_1\sigma_2 &=& 0\\
				\omega(\sigma_1^2 + \sigma_2^2 -2\rho\sigma_1\sigma_2) &=& \sigma_2^2 - \rho\sigma_1\sigma_2
				\end{eqnarray*}
				So we have
					$$\omega^* = \frac{\sigma_2^2 - \rho\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 -2\rho\sigma_1\sigma_2}$$
		\end{solution}
		\part If you want a challenge, check the second order condition from part (e).
			\begin{solution}
							The second derivative is
				$$2\sigma_1^2 - 2\sigma_2^2 - 4\rho \sigma_1\sigma_2$$
				and, since $\rho=1$ is the largest possible value for $\rho$,
				$$2\sigma_1^2 - 2\sigma_2^2 - 4\rho \sigma_1\sigma_2 \geq 2\sigma_1^2 - 2\sigma_2^2 - 4\sigma_1\sigma_2 = 2(\sigma_1 - \sigma_2)^2 \geq 0$$
				so the second derivative is positive, indicating a minimum. This is a global minimum since the problem is quadratic in $\omega$.
			\end{solution}
	\end{parts}
	
	\question Prove that if two random variables are independent, then their covariance is zero.
	\begin{solution}
	Using the definition of covariance, we can get the following (See problem 5):
	\begin{align*}
		Cov(X, Y) &= E[XY] - E[X]E[Y]
	\end{align*}
	Then, we need to examine $E[XY]$
	\begin{align*}
		E[XY] &= \sum_x \sum_y x y p_{XY}(x, y)
		\\
		&= \sum_x \sum_y x p_X(x) y p_Y(y) \text{by independence}
		\\
		&= \sum_x x p_X(x) \sum_y y p_Y(y)
		\\
		&= E[X] E[Y]
	\end{align*}
	
	Hence, we can get that 
	\begin{align*}
		Cov(X, Y) &= E[XY] - E[X]E[Y]
		\\
		&= E[X]E[Y] - E[X]E[Y] = 0
	\end{align*}
	\end{solution}
	\question Prove that expectation of two random variables is linear. $E[aX + bY + c] = a E[X] + bE[Y] + c$
	\begin{solution}
		Using the definition of expectation:
		\begin{align*}
		E[aX + bY + c] &= \int \int (ax + by + c) f_{XY}(x, y) dx dy
		\\
		&= \int \int ax f_{XY}(x, y) dx dy + \int \int by f_{XY}(x, y) dx dy + \int \int c f_{XY}(x, y) dx dy
		\\
		&= a \int x dx \int f_{XY}(x, y) dy + b \int y dy \int f_{XY}(x, y) dx + c \int \int f_{XY}(x, y) dx dy
		\\
		&= a \int x f_{X}(x) dx + b \int y f_{Y}(y) dy + c
		\\
		&= a E[X] + b E[Y] + c
		\end{align*}
	\end{solution}
\end{questions}


\end{document}
