%\documentclass[handout]{beamer}
\documentclass{beamer}
 
\usetheme[numbering = fraction, progressbar = none, background = light, sectionpage = progressbar]{metropolis}
\usepackage{amsmath}
\usepackage{tabu}
\usepackage{graphicx}
\usepackage{tikz}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\title{Econ 103 -- Statistics for Economists}
\subtitle{Chapter 6 and 7: Sampling and Bias}
\author{Mallick Hossain}
\date{}
\institute{University of Pennsylvania}
\begin{document} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\titlepage 
\end{frame} 

\section{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Recall}
	\begin{enumerate}
		\item Sampling Error 
			\begin{itemize}
				\item \emph{Random} differences between sample and population
				\item Cancel out on average
				\item Decreases as sample size grows
			\end{itemize}
		\item Nonsampling Error
			\begin{itemize}
				\item \emph{Systematic} differences between sample and population 
				\item Does \emph{not} cancel out on average
				\item Does \emph{not} decrease as sample size grows
			\end{itemize}
	\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Road Map}
	\begin{enumerate}
		\item We are not going to worry about non-sampling error
		\begin{itemize}
			\item This would be something for a survey design course
		\end{itemize}
		\item We will be learning more about sampling errors
		\begin{itemize}
			\item What do we need to be worried about when sampling from a population? W
		\end{itemize}
	\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Election Polls}
	\includegraphics[scale=0.5]{./images/cnnPoll.jpg}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Who's Right?!?!}
	\includegraphics[scale=0.35]{./images/rcpPoll.jpg}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Building a Bridge Between Probability and Statistics}
  \begin{block}{Questions to Answer}
  \begin{enumerate}
    \item How accurately do sample statistics estimate population parameters?
    \item How can we quantify the uncertainty in our estimates?
  \end{enumerate}
  \end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Election Polls}
  \begin{tikzpicture}
      \node[anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=\textwidth]{./images/cnnPoll.jpg}};
    \draw[red,ultra thick,rounded corners] (8.2,0) rectangle (10.7,0.5);
	\end{tikzpicture}
\end{frame}

\section{Sampling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Step 1: Population as RV rather than List of Objects}
\small
  \begin{tabular}[h]{cc}
    \hline
    \begin{minipage}[t]{0.6\textwidth}
      \begin{block}{Old Way}
       Among 138 million voters, 69 million will vote for Hillary Clinton\\
      \end{block}
    \end{minipage}
    &
    \begin{minipage}[t]{0.4\textwidth}
      \begin{alertblock}{New Way}
       Bernoulli$(p = 1/2)$ RV 
      \end{alertblock}
    \end{minipage} \\
    \hline
    \begin{minipage}[t]{0.6\textwidth}
      \begin{block}{Old Way}
        List of heights for 97 million US adult males with mean 69 in and std.\  dev.\ 6 in \\
      \end{block}
    \end{minipage}
    &
    \begin{minipage}[t]{0.4\textwidth}
      \begin{alertblock}{New Way}
        $N(\mu=69, \sigma^2 = 36)$ RV 
      \end{alertblock}
    \end{minipage} \\
    \hline
  \end{tabular}

  \vspace{1em}
  \alert{Second example assumes distribution of height is bell-shaped.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Random Sample}

\begin{block}{In Words}
Select sample of $n$ objects from  population so that:
	\begin{enumerate}
\item Each member of the population has the same probability of being selected 
\item The fact that one individual is selected does not affect the chance that any other individual is selected
\item Each sample of size $n$ is equally likely to be selected

\end{enumerate}
\end{block}

\begin{alertblock}{In Math}
	$X_1, X_2, \hdots, X_n \sim \mbox{iid } f(x)$ if continuous\\
  $X_1, X_2, \hdots, X_n \sim \mbox{iid } p(x)$ if discrete
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Random Sample Means \emph{Sample With Replacement}}


\begin{itemize}
  \item Without replacement $\Rightarrow$ dependence between samples
  \item Sample small relative to popn.\ $\Rightarrow$ dependence negligible.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Step 2: iid RVs Represent Random Sampling from Popn.}
  \begin{block}{Hillary Clinton Example}
   Poll random sample of 1000 registered voters:
   $$X_1, \hdots, X_{1000} \sim \mbox{ iid Bernoulli}(p = 1/2)$$
  \end{block}
  \begin{block}{Height Example}
   Measure the heights of random sample of 50 US males:
   $$Y_1, \hdots, Y_{50}  \sim \mbox{ iid } N(\mu = 69, \sigma^2 = 36)$$
  \end{block}

  \begin{block}{Key Question}
   What do the properties of the population imply about the properties of the sample? 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What does the population imply about the sample? }
Suppose that exactly half of US voters plan to vote for Hillary Clinton. 
If you poll a random sample of 4 voters, what is the probability that \emph{exactly half} are Hillary supporters? 

\pause

\alert{$${\binom{4}{2}} \left( 1/2 \right)^2 \left( 1/2 \right)^2 = 3/8 = 0.375$$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The rest of the probabilities\dots}
  Suppose that exactly half of US voters plan to vote for Hillary Clinton and we poll a random sample of 4 voters.
  \begin{eqnarray*}
    P\left( \mbox{Exactly 0 Hillary Voters in the Sample} \right) &=& 0.0625\\
    P\left( \mbox{Exactly 1 Hillary Voters in the Sample} \right) &=& 0.25\\
    P\left( \mbox{Exactly 2 Hillary Voters in the Sample} \right) &=& 0.375\\
    P\left( \mbox{Exactly 3 Hillary Voters in the Sample} \right) &=& 0.25\\
    P\left( \mbox{Exactly 4 Hillary Voters in the Sample} \right) &=& 0.0625 
  \end{eqnarray*}

  \vspace{1em}
  \alert{You should be able to work these out yourself. If not, review the lecture slides on the Binomial RV.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Population Size is Irrelevant Under Random Sampling}

  \begin{block}{Crucial Point}
    \emph{None} of the preceding calculations involved the population size: I didn't even tell you what it was!
    We'll never talk about population size again in this course.
  \end{block}

  \begin{block}{Why?}
    Draw with replacement $\implies$ only the sample size and the \emph{proportion} of Hillary supporters in the population matter.
  \end{block}

\end{frame}

\section{Sample Statistics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{(Sample) Statistic}

  Any function of the data \emph{alone}, e.g.\ sample mean $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$. Typically used to estimate an unknown population parameter: e.g.\ $\bar{x}$ is an estimate of $\mu$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Step 3: Random Sampling $\Rightarrow$ \emph{Sample Statistics} are RVs} 

  \alert{This is \emph{the crucial point of the course}: if we draw a random sample, the dataset we get is random. Since a statistic is a function of the data, it is a random variable!} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{A Sample Statistic in the Polling Example}
  Suppose that exactly half of voters in the population support Hillary Clinton and we poll a random sample of 4 voters. If we code Hillary supporters as ``1'' and everyone else as ``0'' then what are the possible values of the sample mean in our dataset?

  \vspace{1em}
  \begin{enumerate}[(a)]
    \item $(0,1)$
    \item $\left\{ 0, 0.25, 0.5, 0.75, 1 \right\}$
    \item $\left\{ 0,1,2,3,4 \right\}$
    \item $(-\infty, \infty)$
    \item Not enough information to determine.
  \end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sampling Distribution}
  Under random sampling, a statistic is a RV so it has a PDF if continuous of PMF if discrete: this is its \alert{sampling distribution}. 

  \begin{block}{Sampling Dist.\ of Sample Mean in Polling Example}
   \begin{eqnarray*}
   p(0) &=&  0.0625\\
   p(0.25) &=&  0.25\\ 
   p(0.5) &=&  0.375\\
   p(0.75)&=& 0.25\\ 
   p(1) &=&  0.0625
   \end{eqnarray*}

  \end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Contradiction? No, but we need better terminology\dots}
  \begin{itemize}
    \item Under random sampling, a statistic is a RV
    \item Given dataset is \emph{fixed} so statistic is a \emph{constant number}
    \item Distinguish between: \alert{Estimator} vs.\ \alert{Estimate}
  \end{itemize}

  \begin{alertblock}{Estimator}
   Description of a general procedure. 
  \end{alertblock}
  \begin{alertblock}{Estimate}
    Particular result obtained from applying the procedure.
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{block}{$\bar{X}_n$ is an Estimator = Procedure = Random Variable}
\begin{enumerate}
\item Take a random sample: $X_1, \hdots, X_n \quad$ 
\item Average what you get: $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\quad$ 
\end{enumerate}
\end{block}

\pause
\begin{block}{$\bar{x}$ is an Estimate = Result of Procedure = Constant}
 \begin{itemize}
\item Result of taking a random sample was the dataset: $x_1, \hdots, x_n$ 
\item Result of averaging the observed data was $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ 
\end{itemize}
\end{block}

\pause
\begin{block}{Sampling Distribution of $\bar{X}_n$}
  \alert{Thought experiment:} suppose I were to repeat the procedure of taking the mean of a random sample over and over \alert{forever}. What \alert{relative frequencies} would I get for the sample means?
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{This is \emph{Only} a Thought Experiment}
  \begin{itemize}
    \item Real applications: observe only a \alert{single} sample:
      \begin{itemize}
        \item $n = $1,189 voters: 44\% Clinton, 43\% Trump, 13\% Undecided.
      \end{itemize}
      \pause
    \item What does the sample tell us about the population?
      \begin{itemize}
        \item How close is Trump's \emph{actual} support to 43\%?
        %\item Why should we choose the voters randomly?
        %\item Is 500 enough people to poll?
      \end{itemize}
      \pause
    \item Can't know for sure without asking \emph{all} voters! 
      \begin{itemize}
        \item Which is impractical and defeats the purpose of the poll!
      \end{itemize}
      \pause
    \item Since we can't be sure, try to \alert{quantify} using \alert{probability}. 
      \begin{itemize}
        \item E.g.\ what is the prob.\ that the poll is off by $>2$\% points?
      \end{itemize}
      \pause
    \item Need to speak in terms of long-run relative frequencies.
      \begin{itemize}
        \item Remember that is the way we define probability in Econ 103!
      \end{itemize}
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Heights of Econ 103 Students}
\begin{center}
\includegraphics[scale = 0.4]{./images/height_hist}
\end{center}
\alert{Use R to illustrate an example where we \emph{know} the population.  
Can't do this in the real applications, but simulate it on the computer\dots}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sampling Distribution of $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$}

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(5,7)
\put(-2,6){\framebox(9,1){Choose 5 Students from Class List with Replacement}}


\put(0.5,6){\vector(-1,-1){1.5}}
\put(-2.3,3.7){\framebox(2.5,0.65){Sample 1}}


\put(-1,3.5){\vector(0,-1){1}}
\put(-1.1,1.9){\makebox{\small $\bar{x}_1$}}

\pause

\put(2,6){\vector(0,-1){1.5}}
\put(0.7,3.7){\framebox(2.5,0.65){Sample 2}}


\put(2,3.5){\vector(0,-1){1}}
\put(1.9,1.9){\makebox{\small $\bar{x}_2$}}

\pause

\put(3.8,4){\makebox{...}}
\put(3.8,2){\makebox{...}}

\pause

\put(4.5,6){\vector(1,-1){1.5}}
\put(4.8,3.7){\framebox(2.5,0.65){Sample M}}


\put(6,3.5){\vector(0,-1){1}}
\put(5.9,1.9){\makebox{\small $\bar{x}_M$}}


\put(-1,1){\makebox{\small Repeat $M$ times $\rightarrow$  get $M$ different sample means}}

\pause

\put(-1.1,0.4){\makebox{\small \alert{Sampling Dist: relative frequencies of the $\bar{x}_i$ when $M = \infty$}}}

\end{picture}
\end{center}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Height of Econ 103 Students}
\begin{figure}
\includegraphics[scale = 0.4]{./images/height_hist}
\includegraphics[scale = 0.4]{./images/height_mean_n_5}
\caption{Left: Population, Right: Sampling distribution of $\bar{X}_5$}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Histograms of sampling distribution of sample mean $\bar{X}_n$}
\alert{Random Sampling With Replacement, 10000 Reps. Each}
\begin{center}
\includegraphics[scale = 0.55]{./images/height_samples}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Population Distribution vs.\ Sampling Distribution of $\bar{X}_n$}

\begin{columns} 
\begin{column}[c]{6cm} 

 %FIRST COLUMN HERE 
\begin{figure}
\centering
\includegraphics[scale = 0.35]{./images/height_hist}
\end{figure}
\end{column} 
\begin{column}[c]{6cm} 

 %SECOND COLUMN HERE 
 \small
\begin{table}
\begin{tabular}{|rrr|}
\hline
&\multicolumn{2}{c|}{Sampling Dist.\ of $\bar{X}_n$}\\
$n$&Mean&Variance\\
\hline
5&67.6&3.6\\
10&67.5&1.8\\
20&67.5&0.8\\
50&67.5&0.2\\
\hline
\end{tabular}
\end{table}

\end{column} 
\end{columns} 
\begin{alertblock}{Two Things to Notice:}
\begin{enumerate}
	\item Sampling dist.\ ``correct on average'' 
	\item Sampling variability decreases with $n$
\end{enumerate}
\end{alertblock}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{$X_1,\hdots, X_{9} \sim \mbox{ iid }$ with $\mu=5$, $\sigma^2 =36$. }

\large Calculate:
	 $$E(\bar{X}) = E\left[\frac{1}{9}(X_1 + X_2 + \hdots + X_{9})\right]$$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Mean of Sampling Distribution of $\bar{X}_n$}
\alert{$X_1, \hdots, X_n \sim \mbox{iid with mean }\mu$}
\begin{eqnarray*}
E[\bar{X_n}] = E\left[ \frac{1}{n}\sum_{i=1}^n X_i \right]\pause = \frac{1}{n} \sum_{i=1}^n E[X_i] = \pause\frac{1}{n} \sum_{i=1}^n \mu = \pause \frac{n\mu}{n} = \mu
\end{eqnarray*}
\alert{Hence, sample mean is ``correct on average.'' The formal term for this is \emph{unbiased}.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{$X_1,\hdots, X_{9} \sim \mbox{ iid }$ with $\mu=5$, $\sigma^2 = 36$. }

\large Calculate:
	 $$Var(\bar{X}) = Var\left[\frac{1}{9}(X_1 + X_2 + \hdots + X_{9})\right]$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Variance of Sampling Distribution of $\bar{X}_n$}
\alert{$X_1, \hdots, X_n \sim \mbox{iid with mean }\mu \mbox{ and variance } \sigma^2$}
\begin{eqnarray*}
Var[\bar{X_n}] &=& Var\left[ \frac{1}{n}\sum_{i=1}^n X_i \right] \pause= \frac{1}{n^2} \sum_{i=1}^n Var(X_i) \\
&=& \pause \frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \pause \frac{n\sigma^2}{n^2} =  \frac{\sigma^2}{n}
\end{eqnarray*}

\alert{Hence the variance of the sample mean \emph{decreases linearly with sample size}.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Standard Error}
Std. Dev.\ of estimator's sampling dist.\ is called \alert{standard error}.
\begin{block}{Standard Error of the Sample Mean}
$SE(\bar{X}_n)= \sqrt{Var\left(\bar{X}_n\right)}= \sqrt{\sigma^2/n}=\sigma/\sqrt{n}$
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{More Generally and More Formally:}
\framesubtitle{Sample mean isn't the only thing with a sampling distribution!}
\begin{block}{Estimator}
  A function $T(X_1, \hdots, X_n)$ of the RVs that represent the \emph{procedure} of drawing a random sample, hence a RV itself.
\end{block}

\begin{block}{Sampling Distribution}
The probability distribution (PMF or PDF) of an Estimator. 
\end{block}

\begin{block}{Estimate}
A function $T(x_1, \hdots, x_n)$ of the \emph{observed data}, i.e.\ the \emph{realizations} of the random variables we use to represent random sampling. 
Since its a function of constants, an estimate is itself a constant.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(5,7)
\put(1,6){\framebox(3,1){Population: $f(x)$}}

\put(4.5,6.5){\makebox{\small \alert{\emph{Probability Distribution}}}}
\put(2.5,6){\vector(0,-1){1}}
\put(0,3){\framebox(5,1){$X_1, X_2, \hdots, X_n \sim \mbox{iid } f(x)$}}
\put(0.5,4.5){\makebox{\small Random Sample of Size $n$}}

\put(-3,3.4){\makebox{\small \alert{\emph{Random Variables}}}}

\put(0.5,3){\vector(-1,-1){1.5}}
\put(-2.3,0.7){\framebox(2.5,0.65){$x_1^{(1)}, \hdots, x_n^{(1)}$}}

\put(2,3){\vector(0,-1){1.5}}
\put(0.7,0.7){\framebox(2.5,0.65){$x_1^{(2)}, \hdots, x_n^{(2)}$}}


\put(3.5,2){\makebox{...}}


\put(4.5,3){\vector(1,-1){1.5}}
\put(4.8,0.7){\framebox(2.5,0.65){$x_1^{(M)}, \hdots, x_n^{(M)}$}}

\put(-1,-0.2){\makebox{\small $M$ Replications, each containing $n$ Observations}}

\put(5.6,2.6){\makebox{\small \alert{\emph{Realizations}}}}
\put(5.6,2.2){\makebox{\small \alert{\emph{(Constants)}}}}

\end{picture}
\end{center}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(5,7)
\put(-1,5){\framebox(4.5,0.6){$X_1, X_2, \hdots, X_n \sim \mbox{iid } f(x)$}}
\put(-0.7,6){\makebox{\small Random Sample of Size $n$}}


\put(-2.7,5.6){\makebox{\small \alert{\emph{Random}}}}
\put(-2.7,5.2){\makebox{\small \alert{\emph{Variables}}}}


\put(3.8,5.3){\vector(1,0){1}}
\put(5.5,6){\makebox{\small Estimator}}
\put(5,5){\framebox(2.5,0.6){$T(X_1,\hdots, X_n)$}}



\put(5,4){\makebox{\small \alert{$\displaystyle\bar{X}_n= \frac{1}{n} \sum_{i=1}^n X_i$}}}





\put(1.2,4.7){\vector(0,-1){2.3}}
\put(-0.2,1.7){\makebox{\small Observations (Data)}}
\put(0,0.7){\framebox(2.5,0.6){$x_1, \hdots, x_n$}}
\put(-2.7,0.9){\makebox{\small \alert{\emph{Constants}}}}


\put(4.5,1.7){\makebox{\small Estimate}}
\put(4,0.7){\framebox(2.5,0.6){$T(x_1, \hdots, x_n)$}}
\put(2.8,1){\vector(1,0){1}}
\put(4,-0.2){\makebox{\small \alert{$\displaystyle\bar{x}= \frac{1}{n} \sum_{i=1}^n x_i$}}}

\end{picture}
\end{center}


\end{frame}

\section{Bias}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Unbiased means ``Right on Average''}

\begin{block}{Bias of an Estimator}
Let $\widehat{\theta}_n$ be a sample estimator of a population parameter $\theta_0$. The \emph{bias} of $\widehat{\theta}_n$ is $E[\widehat{\theta}_n] - \theta_0$.
\end{block}

\begin{block}{Unbiased Estimator}
A sample estimator $\widehat{\theta}_n$ of a population parameter $\theta_0$ is called \emph{unbiased} if $E[\widehat{\theta}_n]= \theta_0$
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Why $(n-1)$ for sample variance?}
\alert{We will show that having $n-1$ in the denominator ensures:}
$$E[S^2] =E\left[ \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \bar{X}\right)^2\right] = \sigma^2$$
\alert{under random sampling.}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Why $(n-1)$ for sample variance?}
\begin{block}{Step \# 1 -- Tedious but straightforward algebra gives:}
	$$\sum_{i=1}^n \left(X_i - \bar{X}\right)^2 = \left[  \sum_{i=1}^n \left(X_i - \mu\right)^2\right] - n(\bar{X}-\mu)^2$$
\end{block}
\alert{You are not responsible for proving Step \#1 on an exam.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\scriptsize
\begin{eqnarray*}
	\sum_{i=1}^n \left(X_i - \bar{X}\right)^2 &=& \sum_{i=1}^n \left(X_i - \mu + \mu - \bar{X}\right)^2 = \sum_{i=1}^n \left[(X_i -\mu) - (\bar{X} - \mu)\right]^2\\
		&=&\sum_{i=1}^n \left[(X_i -\mu)^2 - 2(X_i -\mu)(\bar{X} - \mu) + (\bar{X} - \mu)^2\right]\\
		&=&\sum_{i=1}^n (X_i -\mu)^2 - \sum_{i=1}^n 2(X_i -\mu)(\bar{X} - \mu) + \sum_{i=1}^n (\bar{X} - \mu)^2\\
		&=& \left[  \sum_{i=1}^n \left(X_i - \mu\right)^2\right]   - 2(\bar{X} - \mu) \sum_{i=1}^n (X_i -\mu)+n(\bar{X} - \mu)^2\\
				&=& \left[  \sum_{i=1}^n \left(X_i - \mu\right)^2\right]   - 2(\bar{X} - \mu) \left( \sum_{i=1}^n X_i- \sum_{i=1}^n \mu \right)+n(\bar{X} - \mu)^2\\
				&=& \left[  \sum_{i=1}^n \left(X_i - \mu\right)^2\right]   - 2(\bar{X} - \mu)(n\bar{X}-n\mu)+n(\bar{X} - \mu)^2\\
				&=&\left[  \sum_{i=1}^n \left(X_i - \mu\right)^2\right]   - 2n(\bar{X} - \mu)^2+n(\bar{X} - \mu)^2\\
				&=&\left[  \sum_{i=1}^n \left(X_i - \mu\right)^2\right]   - n(\bar{X} - \mu)^2
\end{eqnarray*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Why $(n-1)$ for sample variance?}
\begin{block}{Step \# 2 -- Take Expectations of Step \# 1:}
	\begin{eqnarray*}
		E\left[\sum_{i=1}^n \left(X_i - \bar{X}\right)^2\right] &=& E\left[\left\{  \sum_{i=1}^n \left(X_i - \mu\right)^2\right\} - n(\bar{X}-\mu)^2\right]\\
			&=&\pause E\left[ \sum_{i=1}^n \left(X_i - \mu\right)^2\right] -E\left[ n(\bar{X}-\mu)^2\right]\\
			&=&\pause  \sum_{i=1}^n E\left[\left(X_i - \mu\right)^2\right] -n\;E\left[ (\bar{X}-\mu)^2\right]\\
	\end{eqnarray*}
\end{block}
\alert{Where we have used the linearity of expectation.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Why $(n-1)$ for sample variance?}
\begin{block}{Step \# 3 -- Use assumption of random sampling:}
\alert{$X_1, \hdots, X_n \sim \mbox{ iid}$ with mean $\mu$ and variance $\sigma^2$}
	\begin{eqnarray*}
		E\left[\sum_{i=1}^n \left(X_i - \bar{X}\right)^2\right] &=&  \sum_{i=1}^n E\left[\left(X_i - \mu\right)^2\right] -n\;E\left[ (\bar{X}-\mu)^2\right]\\
		&=&  \pause \sum_{i=1}^n Var(X_i) -n\;E\left[ (\bar{X}-E[\bar{X}])^2\right]\\
		&=& \pause   \sum_{i=1}^n Var(X_i) -n\;Var(\bar{X})= n\sigma^2 - \sigma^2\\
		&=& \pause (n-1)\sigma^2
	\end{eqnarray*}
\end{block}
\alert{Since we showed earlier today that  $E[\bar{X}]=\mu$ and $Var(\bar{X})=\sigma^2/n$ under this random sampling assumption.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Why $(n-1)$ for sample variance?}
\begin{block}{Finally -- Divide Step \# 3 by $(n-1)$:}
	\begin{eqnarray*}
		E[S^2] &=& E\left[\frac{1}{n-1}\sum_{i=1}^n \left(X_i - \bar{X}\right)^2\right]= \frac{(n-1)\sigma^2}{n-1} = \sigma^2
	\end{eqnarray*}
\end{block}
\alert{Hence, having $(n-1)$ in the denominator ensures that the sample variance is ``correct on average,'' that is \emph{unbiased}.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{A Different Estimator of the Population Variance}

$$\widehat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n \left(X_i - \bar{X}\right)^2$$
\pause
\begin{eqnarray*}
E[\widehat{\sigma}^2 ] = E\left[\frac{1}{n}\sum_{i=1}^n \left(X_i - \bar{X}\right)^2  \right]= \frac{1}{n} E\left[\sum_{i=1}^n \left(X_i - \bar{X}\right)^2\right] = \pause \frac{(n-1)\sigma^2}{n} 
\end{eqnarray*}

\begin{block}{Bias of $\widehat{\sigma}^2$}
\vspace{0.75em}
$\displaystyle E[\widehat{\sigma}^2 ] - \sigma^2 = \pause \frac{(n-1)\sigma^2}{n}  - \sigma^2 = \frac{(n-1)\sigma^2}{n}  - \frac{n\sigma^2}{n} = -\sigma^2/n$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{How Large is the Average Family? }

\Large How many brothers and sisters are in your family, including yourself?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\Large
\alert{The average number of children per family was about 2.0 twenty years ago.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What's Going On Here?}
\pause
Biased Sample!
\begin{itemize}
 	\item  Zero children $\Rightarrow$ didn't send any to college
 	\item Sampling by \emph{children} so large families \alert{oversampled}
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
Let $X_1, X_2, \hdots X_n \sim iid$ mean $\mu$, variance $\sigma^2$ and define $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$. True or False:

\vspace{1em}
\begin{quotation}
$\bar{X}_n$ is an unbiased estimator of $\mu$
\end{quotation}

\begin{enumerate}[(a)]
\item True
\item False
\end{enumerate}
\pause

\alert{TRUE!}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
Let $X_1, X_2, \hdots X_n \sim iid$ mean $\mu$, variance $\sigma^2$. True or False:

\vspace{1em}
\begin{quotation}
$X_1$ is an unbiased estimator of $\mu$
\end{quotation}

\begin{enumerate}[(a)]
\item True
\item False
\end{enumerate}

\pause \alert{TRUE!}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{How to choose between two unbiased estimators?}

Suppose $X_1, X_2, \hdots X_n \sim iid$ with mean $\mu$ and variance $\sigma^2$
\begin{eqnarray*}
E[\bar{X}_n] &=& E\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n}\sum_{i=1}^n E[X_i] = \mu\\
E[X_1] &=& \mu\\
\pause
Var(\bar{X}_n) &=& Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac{1}{n^2} \sum_{i=1}^n Var(X_i) = \alert{\sigma^2/n}\\
\pause
Var(X_1) &=& \alert{\sigma^2}
\end{eqnarray*}
\end{frame}

\section{Efficiency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Efficiency - Compare Unbiased Estimators by Variance}
Let $\widehat{\theta}_1$ and $\widehat{\theta}_2$ be unbiased estimators of $\theta_0$. We say that $\widehat{\theta}_1$ is \alert{\emph{more efficient}} than $\widehat{\theta}_2$ if $Var(\widehat{\theta}_1)<Var(\widehat{\theta}_2)$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Mean-Squared Error}
Except in very simple situations, unbiased estimators are hard to come by. In fact, in many interesting applications there is a \alert{\emph{tradeoff}} between \alert{bias} and \alert{variance}:
\begin{itemize}
\item Low bias estimators often have a high variance 
\item Low variance estimators often have high bias
\end{itemize}
\pause
\vspace{1em}
\alert{Mean-Squared Error (MSE):}   Squared Bias plus Variance
$$ MSE(\widehat{\theta}) = \mbox{Bias}(\widehat{\theta})^2 + Var(\widehat{\theta})$$ 

\alert{Root Mean-Squared Error (RMSE):} $\sqrt{\mbox{MSE}}$  

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Finite Sample versus Asymptotic Properties of Estimators}

\begin{block}{Finite Sample Properties}
For \alert{\emph{fixed sample size $n$}} what are the properties of the sampling distribution of $\widehat{\theta}_n$? (E.g.\ bias and variance.)
\end{block}
\begin{block}{Asymptotic Properties}
What happens to the sampling distribution of $\widehat{\theta}_n$ \alert{\emph{as the sample size $n$ gets larger and larger?}} (That is, $n\rightarrow \infty$).
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Why Asymptotics?}

\begin{block}{Law of Large Numbers}
Make precise what we mean by ``bigger samples are better.'' 
\end{block}
\begin{block}{Central Limit Theorem}
As $n\rightarrow \infty$  \emph{\alert{pretty much any}} sampling distribution is well-approximated by a normal random variable!
\end{block}


\end{frame}

\section{Consistency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Consistency}

\begin{block}{Consistency}
If an estimator $\widehat{\theta}_n$ (which is a RV) \emph{converges} to $\theta_0$ (a constant) as $n\rightarrow \infty$, we say that \emph{\alert{$\widehat{\theta}_n$ is consistent for $\theta_0$}}.
\end{block}
\vspace{2em}

\begin{alertblock}{What does it mean for a \emph{RV} to converge to a \emph{constant?}}
For this course we'll use \emph{MSE Consistency}:
	$$\lim_{n \rightarrow \infty}\mbox{MSE}(\widehat{\theta}_n) = 0$$
This makes sense since MSE$(\widehat{\theta}_n)$ is a \emph{constant}, so this is just an ordinary limit from calculus.
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Law of Large Numbers (aka Law of Averages)}
Let $X_1, X_2, \hdots X_n \sim iid$ mean $\mu$, variance $\sigma^2$. Then the sample mean 
	$$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$$
is consistent for the population mean $\mu$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
\frametitle{Law of Large Numbers (aka Law of Averages)}
Let $X_1, X_2, \hdots X_n \sim iid$ mean $\mu$, variance $\sigma^2$. 
	\begin{eqnarray*}
			E[\bar{X}_n] &=&  E \left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \mu\\\\ 
      Var(\bar{X}_n) &=& Var\left(\frac{1}{n}\sum_{i=1}^n X_i  \right) = \sigma^2/n\\\\ \pause
			\mbox{MSE}(\bar{X}_n) &=& \mbox{Bias}(\bar{X}_n)^2 + Var(\bar{X}_n)\\\pause
				&=& \left(E[\bar{X_n}] - \mu\right)^2 + Var(\bar{X}_n)\\ \pause
				&=& 0 + \sigma^2/n\\ 
				&\rightarrow& 0
	\end{eqnarray*}
	\alert{Hence $\bar{X}_n$ is consistent for $\mu$}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Important! }

An estimator \emph{can} be biased but still consistent, as long as the bias disappears as $n \rightarrow \infty$
$$\widehat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n \left(X_i - \bar{X}\right)^2$$


\begin{block}{Bias of $\widehat{\sigma}^2$}
\vspace{0.75em}
$\displaystyle E[\widehat{\sigma}^2 ] - \sigma^2 = \pause \frac{(n-1)\sigma^2}{n}  - \sigma^2 =  \pause -\sigma^2/n \pause \rightarrow 0$
\end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
Suppose $X_1, X_2, \hdots, X_n \sim \mbox{iid } N(\mu,\sigma^2)$. What is the sampling distribution of $\bar{X}_n$?

\begin{enumerate}[(a)]
\item $N(0,1)$
\item $N(\mu, \sigma^2/n)$
\item $N(\mu, \sigma^2)$
\item $N(\mu/n, \sigma^2/n)$
\item $N(n\mu, n\sigma^2)$
\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\huge\begin{center} But still, how can something random converge to something constant? \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sampling Distribution of $\bar{X}_n$ Collapses to $\mu$}
Look at an example where we can directly calculate not only the mean and variance of the sampling distribution of $\bar{X}_n$, but the \emph{sampling distribution itself}:

\vspace{1em}
$$\alert{X_1, X_2, \hdots, X_n \sim \mbox{iid } N(\mu,\sigma^2) \Rightarrow \bar{X}_n \sim N(\mu, \sigma^2/n)}$$


\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Sampling Distribution of $\bar{X}_n$ Collapses to $\mu$}
\alert{$X_1, X_2, \hdots, X_n \sim \mbox{iid } N(\mu,\sigma^2 \Rightarrow \bar{X}_n \sim N(\mu, \sigma^2/n)$.} \\
\begin{figure}
\centering
\includegraphics[scale = 0.5]{./images/normal_LLN_dist}
\caption{Sampling Distributions for $\bar{X}_n$ where $X_i \sim \mbox{iid } N(0,1)$}
\end{figure}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Another Visualization: Keep Adding Observations}
\begin{columns} 
\begin{column}[c]{8cm} 
 %FIRST COLUMN HERE 
\begin{figure}
\centering
\includegraphics[scale = 0.5]{./images/WLLN}
\caption{Running sample means: $X_i \sim \mbox{iid } N(0, 100)$}
\end{figure}
\end{column} 
\begin{column}[c]{3cm} 

 %SECOND COLUMN HERE 
\footnotesize
\begin{table}
\begin{tabular}{|rr|}
\hline
$n$&$\bar{X}_n$\\
\hline
1 &-2.69\\
2 &-3.18\\
3 &-5.94\\
4 &-4.27\\
5 &-2.62\\
10& -2.89\\
20& -5.33\\
50 &-2.94\\
100& -1.58\\
500 &-0.45\\
1000& -0.13\\
5000& -0.05\\
10000&  0.00\\
\hline
\end{tabular}
\end{table}

\end{column} 
\end{columns} 


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
