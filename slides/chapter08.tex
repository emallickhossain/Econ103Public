%\documentclass[handout]{beamer}
\documentclass{beamer}
 
\usetheme[numbering = fraction, progressbar = none, background = light, sectionpage = progressbar]{metropolis}
\usepackage{amsmath}
\usepackage{tabu}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{setspace}
\usetikzlibrary{shapes,backgrounds,trees}

\title{Econ 103 -- Statistics for Economists}
\subtitle{Chapter 8: Hypothesis Testing}
\author{Mallick Hossain}
\date{}
\institute{University of Pennsylvania}
\begin{document} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\titlepage 
\end{frame} 

\section{The Lady Tasting Tea}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{An excerpt from \emph{The Lady Tasting Tea} by David Salsburg}
\footnotesize
\begin{quote}
It was a summer afternoon in Cambridge, England, in the late 1920s. A group of university dons, their wives, and some guests were sitting around an outdoor table for afternoon tea. One of the women was insisting that tea tasted different depending upon whether the tea was poured into the milk or whether the milk was poured into the tea. The scientific minds among the men scoffed at this as sheer nonsense. What could be the difference? They could not conceive of any difference in the chemistry of the mixtures that could exist. A thin, short man, with thick glasses and a Vandyke beard beginning to turn gray, pounced on the problem. ``Let us test the proposition'' he said excitedly. He began to outline an experiment in which the lady who insisted there was a diference would be presented with a sequence of cups of tea, in some of which the milk had been poured into the tea and in others of which the tea had been poured into the milk.
\end{quote}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}

 \begin{figure}
 \includegraphics[scale = 0.35]{./images/grant1}

\caption{The Orchard, Grantchester}
 \end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}

\begin{figure}
 \includegraphics[scale = 0.15]{./images/grant2}

\caption{What to have with your tea.}
 \end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

\begin{figure}
 \includegraphics[scale = 0.31]{./images/grant6}\\
\vspace{0.6em}
 \includegraphics[scale = 0.211]{./images/grant7}

\caption{Why walk when you can punt?}
 \end{figure}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}

\begin{figure}
 \includegraphics[scale = 0.3]{./images/grant8}

\caption{What to wear.}
 \end{figure}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Continued...}
\footnotesize
\begin{quote}
And so it was that summer afternoon in Cambridge. The man with the Vandyke beard was Ronald Aylmer Fisher, who was in his late thirties at the time. He would later be knighted Sir Ronald Fisher. In 1935, he wrote a book entitled The Design of Experiments, and he described the experiment of the lady tasting tea in the second chapter of that book. In his book, Fisher discusses the lady and her belief as a hypothetical problem. He considers the various ways in which an experiment might be designed to determine if she could tell the difference.
\end{quote}
\end{frame}

\section{The Pepsi Challenge}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Pepsi Challenge}
Our expert claims to be able to tell the difference between Coke and Pepsi. Let's put this to the test! 
\begin{itemize}
\item Eight cups of soda 
	\begin{itemize}
\item Four contain Coke 
\item Four contain Pepsi 
\end{itemize}
	\item The cups are randomly arranged 
	\item How can we use this experiment to tell if our expert can \emph{\alert{really}} tell the difference?
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Results:}
	\# of Cokes Correctly Identified: \\ \vspace{2em}
	\alert{What do you think? Can our expert really tell the difference? }
		\begin{enumerate}[(a)]
\item Yes
\item No
\end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
If you just guess randomly, what is the probability of identifying \emph{all four cups of Coke correctly}?
\pause
\begin{itemize}
\item ${8\choose 4}=70$ ways to choose four of the eight cups. \pause
\item If guessing randomly, each of these is \emph{\alert{equally likely}} \pause
\item Only \emph{\alert{one}} of the 70 possibilities corresponds to correctly identifying all four cups of Coke. \pause
\item Thus, the probability is $1/70 \approx 0.014$
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
If you just guess randomly, what is the probability of identifying \emph{all but one cup of Coke correctly}?
\pause
\begin{itemize}
\item ${8\choose 4}=70$ ways to choose four of the eight cups. \pause
\item If guessing randomly, each of these is \emph{\alert{equally likely}} \pause
\item There are 16 ways to mis-identify one Coke: 
	\begin{itemize}
		\item $4$ choices of \emph{which} Coke you call a Pepsi 
		\item $4$ choices of \emph{which} Pepsi you call a Coke 
		\item Total of $4\times 4 = 16$ possibilities \pause
	\end{itemize}	
\item Thus, the probability is $16/70 \approx 0.23$
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Probabilities if Guessing Randomly}
	\begin{center}
		\begin{tabular}{rccccc}
		\hline
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\begin{center}
		\begin{tabular}{rccccc}
		\hline
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
	If you're just guessing, what is the probability of identifying \alert{\emph{at least}} three Cokes correctly?
	\pause
	\begin{itemize}
\item Probabilities of mutually exclusive events sum. 
\item $P$(all four correct) = 1/70 
\item $P$(exactly 3 correct )= 16/70 
\item $P$(at least three correct) $ = 17/70 \approx 0.24$

\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Pepsi Challenge}
	\begin{itemize}
\item Even if you're just guessing randomly, the probability of correctly identifying three or more Cokes is around 24\% 
\item In contrast, the probability of identifying \emph{\alert{all four}} Cokes correctly is only around 1.4\% if you're guessing randomly. 
\item We should probably require the expert to get them all right. 
\item What if the expert gets them all wrong? This also has probability $1.4\%$ if you're guessing randomly...
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\begin{center}
	\huge That was a Hypothesis Test!\\
	\normalsize We'll go through the details in a moment, but first an analogy...
	\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
	\begin{center}
	\huge Hypothesis Testing is Similar to a Criminal Trial
	\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
%\frametitle{Hypothesis Testing -- Analogy to Criminal Trial}
\footnotesize
\begin{columns}
\begin{column}{6cm} 
 %FIRST COLUMN HERE
   	\begin{block}{Criminal Trial}
	\begin{itemize}
		\item<1-> The person on trial is either innocent or guilty (but not both!)
		\item<2-> ``Innocent Until Proven Guilty''
		\item<3-> Only convict if evidence is ``beyond a shadow of a doubt''
		\item<4-> \emph{Not Guilty} rather than Innocent
			\begin{itemize}\footnotesize
				\item<5-> Acquit $\neq$ Innocent
			\end{itemize}
		\item<6-> Two Kinds of Errors:
			\begin{itemize} \footnotesize
				\item<6-> Convict the innocent
				\item<6->  Acquit the guilty
			\end{itemize}
		\item<7-> Convicting the innocent is a worse error. Want this to be rare even if it means acquitting the guilty.	\end{itemize}
\end{block}
   
   
\end{column} 
\begin{column}{6cm} 

 %SECOND COLUMN HERE 

\begin{block}{Hypothesis Testing}
		\begin{itemize}
		\item<1-> Either the null hypothesis $H_0$ or the alternative $H_1$  hypothesis is true.
		\item<2-> Assume $H_0$ to start
		\item<3-> Only reject $H_0$ in favor of $H_1$ if there is strong evidence.
		\item<4-> \emph{Fail to reject} rather than Accept $H_0$	
		\begin{itemize} \footnotesize
			\item<5-> (Fail to reject $H_0) \neq (H_0$ True) 
		\end{itemize}
				\item<6-> Two Kinds of Errors:
			\begin{itemize} \footnotesize
				\item<6-> Reject true $H_0$ (Type I)
				\item<6-> Don't reject false $H_0$ (Type II)
			\end{itemize}
			\item<7-> Type I errors (reject true $H_0$) are worse: make them rare even if that means more Type II errors.
	\end{itemize}
\end{block}

\end{column} 
\end{columns} 

\end{frame}

\section{Hypothesis Testing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{How is the Pepsi Challenge a Hypothesis Test?}
	\begin{block}{Null Hypothesis $H_0$}
		Can't tell the difference between Coke and Pepsi: just guessing. \pause
\end{block}
	\begin{block}{Alternative Hypothesis $H_1$}
	Able to distinguish Coke from Pepsi.\pause
\end{block}
	\begin{block}{Type I Error -- Reject $H_0$ even though it's true} 
	Decide expert can tell the difference when she's really just guessing. \pause
\end{block}
	\begin{block}{Type II Error -- Fail to reject $H_0$ even though it's false}
	Decide expert just guessing when she really can tell the difference. 
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{How do we find evidence to reject $H_0$?}
	\begin{itemize}
		\item Choose a \alert{significance level $\alpha$} maximum probability of Type I error that we are willing to tolerate. 
			\begin{itemize}
				\item Measures how often we will reject a true null, i.e.\ convict an innocent person \pause
			\end{itemize}
		\item Test Statistic $T_n$ uses sample to measure plausibility of $H_0$ \pause
		\item Null Hypothesis $H_0 \Rightarrow$ Sampling Distribution for $T_n$  
			\begin{itemize}
				\item ``Under the null'' means ``assuming the $H_0$ is true'' \pause
			\end{itemize}
		\item Using $\alpha$ and the sampling distribution of $T_n$ under the null, we construct a \alert{decision rule} in terms of a critical value $c_\alpha$ \pause
			\begin{itemize}
				\item Reject $H_0$ if $T_n > c_\alpha$
			\end{itemize}
	\end{itemize}
	
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{frame}
 \frametitle{We still have a random sampling model in mind!}
 \footnotesize
 \begin{block}{Why does $T_n$ have a sampling distribution?}
 	\begin{itemize}
 		\item Random Sampling: new data $\Rightarrow$ different \emph{realization} $t$ of $T_n$ 
 		\item Key point: $T_n$ is a \emph{random variable} with a particular distribution under the null hypothesis $H_0$ \pause
 	\end{itemize}
 \end{block}

\begin{block}{What do we mean by $\alpha$?} 
 	\begin{itemize}
 		\item $T_n$ is a RV $\Rightarrow$ outcome of hypothesis test is random! 
 		\item Sometimes we make mistake: either reject $H_0$ when it is true or fail to reject it when it is false. 
 		\item Repeated Sampling $\Rightarrow$ many different realizations of $T_n\Rightarrow$ many different outcomes of the test. 
 		\item Test is constructed so that, if $H_0$ is true, we will reject it no more than $100\times \alpha \%$ of the time under repeated sampling.
 	\end{itemize}
 \end{block}
 \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Pepsi Challenge}
	\begin{block}{Test Statistic $T_n$}
		$T_n =$ Number of Cokes correctly identified
\end{block} 
	\begin{block}{$H_0\colon$ No skill, just guessing randomly}
	Under this null hypothesis, the sampling distribution of $T_n$ is:
		\begin{center}
		\begin{tabular}{rccccc}
		\hline
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
\end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Pepsi Challenge }
$T_n\colon$ \# of Cokes correctly identified. Sampling Dist.\ under $H_0$:
		\begin{center}
		\begin{tabular}{rccccc}
		\hline
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
	\alert{If I choose a significance level of $\alpha =0.05$, what critical value should I use?}\\ (Remember that $\alpha$ is the probability of rejecting $H_0$ when it is actually true.)
	\pause
	
	\vspace{2em}
	Want $P(\mbox{Reject } H_0|H_0 \mbox{ True})\leq 0.05$\\ 
	$P(T_n \geq 3 |\mbox{Just Guessing}) = 17/70 \approx 0.23 > 0.05$ 
	$P(T_n \geq 4 |\mbox{Just Guessing}) = 1/70 \approx 0.014 \alert{\leq 0.05}$ 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Pepsi Challenge }
$T_n\colon$ \# of Cokes correctly identified. Sampling Dist.\ under $H_0$:
		\begin{center}
		\begin{tabular}{rccccc}
		\hline
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
	\alert{If I choose a significance level of $\alpha =0.25$, what critical value should I use?}
	\pause
	
	\vspace{2em}
	Want $P(\mbox{Reject } H_0|H_0 \mbox{ True})\leq 0.25$\\ 
	$P(T_n \geq 2 |\mbox{Just Guessing}) = 53/70 \approx 0.76 > 0.25$ 
	$P(T_n \geq 3 |\mbox{Just Guessing}) = 17/70 \approx 0.23 \alert{\leq 0.25}$ 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Pepsi Challenge }
\footnotesize 
$H_0\colon$ Expert is just guessing randomly.\\
$H_1\colon$ Expert can distinguish Coke from Pepsi.\\
$T_n\colon$ \# of Cokes correctly identified. Has following sampling under the null:
		\begin{center}
		\begin{tabular}{rccccc}
		\hline \footnotesize
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
	\vspace{2em}
	\normalsize
	\alert{If I choose $\alpha =0.05$, what decision rule should I use?}
	\begin{enumerate}[(a)]
		\item Reject $H_0$ if $T_n \geq 0$
		\item Reject $H_0$ if $T_n \geq 1$
		\item Reject $H_0$ if $T_n \geq 2$
		\item Reject $H_0$ if $T_n \geq 3$
		\item Reject $H_0$ if $T_n \geq 4$
	\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Pepsi Challenge}
\footnotesize 
$H_0\colon$ Expert is just guessing randomly.\\
$H_1\colon$ Expert can distinguish Coke from Pepsi.\\
$T_n\colon$ \# of Cokes correctly identified. Has following sampling under the null:
		\begin{center}
		\begin{tabular}{rccccc}
		\hline \footnotesize
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
	\vspace{2em}
	\normalsize
	\alert{If I choose $\alpha =0.05$, what decision rule should I use?}\\
\vspace{1em}
	Need $P(\mbox{Reject } H_0|H_0 \mbox{ True})\leq \alpha = 0.05$ 
	\begin{eqnarray*}
		P(T_n \geq 3 |\mbox{Just Guessing}) &=& 17/70 \approx 0.23 > 0.05 \\
	P(T_n \geq 4 |\mbox{Just Guessing}) &=& 1/70  \approx 0.014 \alert{\leq 0.05}
	\end{eqnarray*}
	\vspace{1em}
	\alert{Critical value for $\alpha = 0.05$ is 4}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Example: Pepsi Challenge }
\footnotesize 
$H_0\colon$ Expert is just guessing randomly.\\
$H_1\colon$ Expert can distinguish Coke from Pepsi.\\
$T_n\colon$ \# of Cokes correctly identified. Has following sampling under the null:
		\begin{center}
		\begin{tabular}{rccccc}
		\hline \footnotesize
		\# Correct & 0 & 1 & 2 & 3 & 4\\
		Prob.&1/70 & 16/70 & 36/70 & 16/70 &1/70\\
		\hline
		\end{tabular}
	\end{center}
	\vspace{2em}
	\normalsize
	\alert{If I choose $\alpha =0.25$, what critical value should I use?}
	\begin{enumerate}[(a)]
		\item 0
		\item 1
		\item 2
		\item 3
		\item 4
	\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Hypothesis: Assertion about Population(s)}
	\begin{itemize}
	\item A Big Mac contains, on average, 550 kcal: \alert{$\mu = 550$}
	\item Midterm 2 was harder than Midterm 1: \alert{$\mu_{1} >\mu_2$}
	\item Equal proportions of Republicans and Democrats know that John Roberts is the chief justice of SCOTUS: \alert{$p = q$}
	\item Google stock is riskier than IBM stock: \alert{$\sigma^2_{X} > \sigma^2_{Y}$}
	\item There is no correlation between height and income: \alert{$\rho = 0$} 
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Hypothesis Testing: Try to 
	Find Evidence \emph{Against} $H_0$}
\begin{block}
	{Null Hypothesis: $H_0$}
	\begin{itemize}
		\item Start off assuming $H_0$ is true --
		 ``innocent until proven guilty''
		\item ``Under the Null'' = Assuming the null is true
		\item $H_0$ $\Rightarrow$ know something about population, can calculate probs.
	\end{itemize}
\end{block}
\begin{alertblock}
	{This Course: \emph{Simple} Null Hypotheses}
	$H_0\colon f(\mbox{Parameters}) = \mbox{Known Constant}$, for example
	\begin{itemize}
		\item $\mu_1 - \mu_2 = 0$
		\item $p = 0.5$
		\item $\mu = 0$
		\item $\sigma^2_X/\sigma^2_Y = 1$
	\end{itemize}
\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{How do I know what my null hypothesis is?}
	There is no rule I can give you for this: it depends on the problem. Here are some guidelines:
	\begin{itemize}
		\item It will take the form $f(\mbox{Parameters}) = \mbox{Known Constant}$
		\item Nulls are typically things like ``there is no effect,'' ``these two groups are not different,'' i.e.\ the \emph{status quo}. 
		\item Nulls are \emph{very specific}: we need to be able to do probability calculations under the null -- c.f.\ the Pepsi Challenge.
	\end{itemize}
\end{frame}

\section{Big Mac Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
	\frametitle{Example: How many calories in a Big Mac? }
\begin{itemize}
	\item According to McDonald's: 550 kcal on average
	\item Measure calories in random sample of $9$ Big Macs: $X_1, \hdots, X_{9} \sim \mbox{iid } N(\mu, \sigma^2)$
\end{itemize}

\vspace{1em}

\alert{If we wanted to test McDonald's claim, what would be $H_0$?}
\begin{enumerate}[(a)]
	\item $\sigma^2 = 1$
	\item $\mu = 0$
	\item $\mu > 550$ 
	\item $\mu = 550$
	\item $\mu \neq 550$
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
	\frametitle{Example: How many calories in a Big Mac? }
\begin{itemize}
	\item According to McDonald's: 550 kcal on average
	\item Measure calories in random sample of $9$ Big Macs: $X_1, \hdots, X_{9} \sim \mbox{iid } N(\mu, \sigma^2)$
\end{itemize}

\vspace{1em}

\alert{If McDonald's is telling the truth, approximately what value should we get for the sample mean caloric content of the $9$ Big Macs?} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
	\frametitle{Example: How many calories in a Big Mac? }
\begin{itemize}
	\item According to McDonald's: 550 kcal on average
	\item Measure calories in random sample of $9$ Big Macs: $X_1, \hdots, X_{9} \sim \mbox{iid } N(\mu, \sigma^2)$
\end{itemize}

\vspace{1em}

\alert{If the sample mean does not equal 550, does this prove that McDonald's is lying?}
\begin{enumerate}[(a)]
	\item Yes
	\item No
	\item Not Sure
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{How to find evidence against $H_0$? Test Statistic!}
	\begin{block}
		{Test Statistic: $T_n$}
		A statistic that gives us information about the parameter we are testing and has a \emph{known} sampling distribution \emph{under $H_0$}.
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
	\frametitle{Example: How many calories in a Big Mac? }
\begin{itemize}
	\item Measure calories in random sample of $n$ Big Macs: $X_1, \hdots, X_9 \sim \mbox{iid } N (\mu, \sigma^2)$
	\item $H_0\colon \mu = 550$
\end{itemize}

\vspace{1em}

\alert{If McDonald's is telling the truth, i.e.\ under the null, what is \emph{exact}  sampling distribution of $(\bar{X} - 550)/(S/3)$?}
\begin{enumerate}[(a)]
	\item $\chi^2_{9}$
	\item $N(550, 1)$
	\item $F(9, 1)$
	\item $N(0,1)$ 
	\item $t_{8}$
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{What if the null is false?}

	\begin{block}
		{Alternative hypothesis: $H_1$}
		The \emph{negation} of the null hypothesis.
	\end{block}
	\begin{block}
		{Examples:}
		\begin{enumerate}
			\item 
				\begin{itemize}
					\item $H_0\colon$ This parameter equals 5.
				\item $H_1\colon$ This parameter does \emph{not} equal 5.
		\end{itemize}
			\item 
				\begin{itemize}
					\item $H_0\colon$ There is no difference between these two groups.
					\item $H_1\colon$ There \emph{is} a difference between these two groups.
				\end{itemize}
		\end{enumerate}
		
	\end{block}

	\alert{Sometimes we only care about \emph{certain kinds} of violations of $H_0$...}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{One-sided vs.\ Two-sided Alternative}
\alert{Let $\theta$ be a population parameter and $\theta_0$ be a specified constant.}
\begin{block}
	{Null Hypothesis}
\begin{itemize}
	\item $H_0\colon \theta = \theta_0$
\end{itemize}\end{block}
	\begin{block}{Two-sided Alternative}
		\begin{itemize}
			\item $H_1\colon \theta \neq \theta_0$
		\end{itemize}
\end{block}
	\begin{block}{One-sided Alternative}
		Two possibilities, depending on the problem at hand:
		\begin{itemize}
			\item $H_1\colon \theta > \theta_0$
			\item $H_1\colon \theta < \theta_0$
		\end{itemize}
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Suing McDonald's }

A class action lawsuit claims that McDonald's has been  understating the caloric content of the ``Big Mac,'' misleading consumers into thinking the sandwich is healthier than it really is. McDonald's claims the sandwich contains $550$ kcal on average. \\

\vspace{1em}
\alert{Suppose you're the judge in this case. What is your alternative hypothesis?}

	\begin{enumerate}[(a)]
		\item $H_1\colon \mu \neq 550$ kcal
		\item $H_1\colon \mu < 550$ kcal
		\item $H_1\colon \mu > 550$ kcal
		\item $H_1\colon \mu = 550$ kcal
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Example: Quality Control at McDonald's }

You are a senior manager at McDonald's and are concerned that franchises may be deviating from company policy on the calorie count of a Big Mac sandwich, which is supposed to be 550 kcal on average. Because intervening is costly, you will only take action is there is strong evidence of deviation from company policy. \\

\vspace{1em}

\alert{What is your alternative hypothesis?}
	\begin{enumerate}[(a)]
		\item $H_1\colon \mu \neq 550$ kcal
		\item $H_1\colon \mu < 550$ kcal
		\item $H_1\colon \mu > 550$ kcal
		\item $H_1\colon \mu = 550$ kcal
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Decision Rule: When should we reject $H_0$?}
	\begin{itemize}
		\item Test statistic: RV with known sampling distribution under $H_0$
		\item McDonald's Example: $T_n = 3(\bar{X} - 550)/S$
		\item \emph{Random} since $\bar{X}$ and $S$ are RVs under random sampling: functions of $X_1, \hdots, X_9$.
		\item Observed dataset: \emph{realizations} $x_1, \hdots, x_9$ of RVs $X_1, \hdots, X_9$
		\item Plug in observed data to get estimates (constants) $\bar{x}$ and $s$.
		\item Plug these into the formula for the test statistic to get a \emph{number} -- this is a \emph{realization} of $T_n$ 
		\item Depending on this number, decide whether to reject $H_0$.
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What Form Should the Decision Rule Take?}
\begin{columns}
\begin{column}{6cm}
\includegraphics[scale = 0.5]{./images/t_pdf}
\end{column}

\begin{column}{6cm}
$H_0\colon \mu=550 \Rightarrow \displaystyle \frac{\bar{X} - 550}{S/3} \sim t(8)$\\ \pause
\vspace{1em}
One-sided Alternative $H_1\colon \mu > 550$\\ \pause
\vspace{1em}
Two-sided Alternative $H_1\colon \mu \neq 550$ 
\end{column}

\end{columns}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Suing McDonald's }
The plaintiffs allege that McDonald's has \emph{understated} the true caloric content of a Big Mac: it's actually \emph{greater} than 550 kcal. \alert{Suppose the plaintiffs are right. Then what sort of value should we expect the test statistic $3(\bar{X} - 550)/S$ to take on?}

\vspace{1em}
\begin{enumerate}[(a)]
	\item A value \emph{less} than zero.
	\item A value close to zero.
	\item A value \emph{greater} than zero.
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Quality Control at McDonald's }
The senior manager is worried that franchises are deviating from company policy that Big Macs should contain approximately 550 kcal. \alert{If the franchises \emph{are} deviating, what sort of of value should we expect the test statistic $3(\bar{X} - 550)/S$ to take on?}

\vspace{1em}
\begin{enumerate}[(a)]
	\item A value \emph{less} than zero.
	\item A value close to zero.
	\item A value \emph{greater} than zero.
	\item A value different from zero but we can't tell whether it will be positive or negative.
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What Form Should the Decision Rule Take?}
$X_1, \hdots, X_n \sim \mbox{iid } N(\mu, \sigma^2)$ 
\begin{block}{Common Null Hypothesis $H_0\colon \mu = 550$}
Under $H_0$, $T_n = \sqrt{n}(\bar{X}_n - 550)/S \sim t(n-1)$ 
\end{block}
\begin{block}{One-sided Alternative $H_1\colon \mu > 550$}
Reject $H_0$ if $T_n$ is ``too big'' 
\end{block}
\begin{block}{Two-sided Alternative $H_1\colon \mu \neq 550$} 
Reject $H_0$ if $T_n$ is ``too big'' or ``too small''
\end{block}

\vspace{1em}

\alert{But how big of a discrepancy is ``big enough'' to reject?}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Two Kinds of Mistakes in Hypothesis Testing}
	\begin{block}
		{Type I Error}
		\begin{itemize}
			\item Rejecting the null when it's actually true.
			\item $P(\mbox{Type I Error}) = \alpha\quad \quad$ 
			\alert{$\boxed{\alpha= \mbox{``Significance Level'' of Test}}$}
		\end{itemize}
	\end{block}
	 \begin{block}
		{Type II Error}
		\begin{itemize}
			\item Failing to reject the null when it's false.
			\item $P(\mbox{Type II Error}) = \beta \quad \quad$ 
			\alert{$\boxed{1 - \beta= \mbox{``Power'' of Test}}$}
		\end{itemize}
	\end{block}
	\begin{alertblock}
		{Important!}
		Hypothesis testing \emph{controls} probability of a Type I error since this is assumed to be the \emph{worse} kind of mistake: convicting the innocent.	
	\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Construct a Decision Rule to \emph{Fix} $\alpha$ at User-Chosen Level}

	\begin{block}
		{Critical Value $c_{\alpha}$} 
	\begin{itemize}
		\item Threshold for rejecting $H_0$
		\item Chosen so that $P(\mbox{Reject } H_0|H_0 \mbox{ is True}) = \alpha$
		\item Depends on \emph{both} $\alpha$ \emph{and} the alternative hypothesis.
	\end{itemize}
	\end{block}
	\begin{block}
		{One-Sided Alternative}
		Reject $H_0$ if $T_n >$ Critical Value
	\end{block}
	\begin{block}
		{Two-Sided Alternative}
		Reject $H_0$ if $|T_n| >$ Critical Value
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: One-sided Alternative $H_1\colon \mu > 550$}
The critical value is chosen to reflect both the alternative hypothesis and the significance level. 
\begin{figure}
\includegraphics[scale = 0.45]{./images/one_side}
\end{figure}
One-sided Critical Value: \texttt{qt($1-\alpha$, df  = $n-1$)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Example: Two-sided Alternative $H_1\colon \mu \neq 550$}
The critical value is chosen to reflect both the alternative hypothesis and the significance level. 
\begin{figure}
\includegraphics[scale = 0.45]{./images/two_side}
\end{figure}
Two-sided Critical Value: \texttt{qt($1-\alpha/2$, df  = $n-1$)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
Suppose, for example, $\alpha = 0.05$, $n = 9$
	\begin{eqnarray*}
		&&\texttt{qt(0.95, df  = 8)}\approx 1.86\\
		 &&\texttt{qt(0.975, df  = 8)}\approx 2.3
	\end{eqnarray*}
\begin{figure}
\includegraphics[scale = 0.3]{./images/one_side}
\includegraphics[scale = 0.3]{./images/two_side}
\end{figure}
One-sided Alternative: Reject $H_0$ if $3(\bar{X}_n - 550)/S \geq 1.86$\\
\vspace{0.5em}
Two-sided Alternative: Reject $H_0$ if $\left|3(\bar{X}_n - 550)/S\right| \geq 2.3$\\

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{McDonald's Example}
Suppose $n=9$, $\bar{x} = 563$, $s = 34$. What is  the value of our test statistic?

\pause
\vspace{1em}
	$$\frac{563 - 550}{34/\sqrt{9}}= \frac{13}{34/3} \approx 1.14$$


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
\frametitle{McDonald's Example: $\alpha = 0.05$}
Recall that:
\begin{eqnarray*}
		&&\texttt{qt(0.95, df  = 8)}\approx 1.86\\
		 &&\texttt{qt(0.975, df  = 8)}\approx 2.3
	\end{eqnarray*}
Based on an observed test statistic of $1.14$, would we reject $H_0$ against the one-sided alternative at the 5\% significance level?
\begin{enumerate}[(a)]
	\item Yes
	\item No
	\item Not Sure
\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
\frametitle{McDonald's Example: $\alpha = 0.05$}
Recall that:
\begin{eqnarray*}
		&&\texttt{qt(0.95, df  = 8)}\approx 1.86\\
		 &&\texttt{qt(0.975, df  = 8)}\approx 2.3
	\end{eqnarray*}
Based on an observed test statistic of $1.14$, would we reject $H_0$ against the \alert{two-sided} alternative at the 5\% significance level?
\begin{enumerate}[(a)]
	\item Yes
	\item No
	\item Not Sure
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Reporting the Results of a Hypothesis Test}
	\begin{block}
		{Lawsuit Example}
		The judge \emph{failed to reject} the null hypothesis that $\mu = 550$ against the one-sided alternative $\mu > 550$ at the 5\% significance level.
	\end{block}
	\begin{block}
		{Quality Control Example}
		The senior manager \emph{failed to reject} the null hypothesis that $\mu =550$ against the two-sided alternative at the 5\% significance level.
	\end{block}
	\begin{block}
		{Interpretation}
		In each of these two cases, there was \emph{insufficient evidence} the initial assumption that $\mu = 550$ \emph{given the significance level used}.
	\end{block}
	\alert{But what if we have used a \emph{different} significance level?}
\end{frame}

\section{P-Values}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{The P-Value of a Hypothesis Test}
	\begin{block}
		{Two Equivalent Definitions:}
		\begin{enumerate}
			\item Given the value we calculated for our test statistic, what is the \emph{smallest $\alpha$} at which we would have rejected the null?
			\item Under the null, what is the probability of observing a test statistic \emph{at least as extreme} as the one we \emph{actually} observed?
		\end{enumerate}
	\end{block}
	\begin{block}
		{Why Report P-Values?}
		\begin{itemize}
			\item More informative than reporting $\alpha$ and Reject/Fail to Reject
			\item E.g. a p-value of 0.03 means we would have rejected the null for any $\alpha \geq 0.03$ and failed to reject it for any $\alpha < 0.03$ 
		\end{itemize}
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\begin{center}
\huge P-Value Depends on Which Alternative We Have Specified!
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (One-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the one-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_upper1}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (One-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the one-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_upper2}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (One-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the one-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_upper3}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (One-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the one-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_upper4}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (One-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the one-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_upper4}

\end{figure}
\texttt{1 - pt(1.14, df = 8)}$\approx 0.14$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (Two-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the two-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_both1}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (Two-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the two-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_both2}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (Two-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the two-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_both3}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (Two-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the two-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_both4}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (Two-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the two-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_both5}

\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is the p-value? (Two-sided Test)}
\footnotesize
Recall: p-value is \emph{smallest significance level} at which our observed test statistic would cause us to reject $H_0$. \alert{Test statistic is $1.14$. What is the two-sided p-value? }
\begin{figure}
\includegraphics[scale= 0.4]{./images/p_both5}
\end{figure}

\texttt{2 * pt(-1.14, df = 8)}$\approx 0.28$ \pause \hfill \alert{This is twice the one-sided p-value!}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Two-sided Test is More Stringent}
\begin{block}{P-value measures strength of evidence against $H_0$}
Lower p-value means stronger evidence. 
\end{block}

\begin{block}{(Two-sided p-value) $= 2 \; \times$  (one-sided p-value)}
Reject $H_0$ based on two-sided test $\implies$ Reject $H_0$ based on appropriate one-sided test. The converse is \emph{false}.
\end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Steps in Hypothesis Testing}

\begin{enumerate}
\item Specify Null and Alternative Hypotheses
\item Identify a Test Statistic: a function of the data that has a known sampling distribution under the null.
\item Specify a Decision Rule and a Critical Value so the Type I Error Rate equals $\alpha$.
\end{enumerate}

\begin{alertblock}{Alternative to Step 3}
	Calculate P-Value: the minimum significance level  ($\alpha$) at which we would reject $H_0$ given the observed data.
\end{alertblock}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{How to Handle Other Examples?}

\alert{You already know lots of sampling distributions! Testing is very similar to constructing confidence intervals in that the steps are always the same, and the only thing that differs is \emph{which} sampling distribution we work with.}

\end{frame}

\section{Hypothesis Testing and Confidence Intervals}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Relationship between CI and Two-Sided Test}

	\begin{itemize}
		\item There is a \emph{very close} relationship between CIs and hypothesis tests against a two-sided alternative.
		\item I'll illustrate this using a generic version of the example from last class but the relationship holds \emph{in general}.
	\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Relationship between CI and Two-sided Test}
Suppose $X_1, \hdots, X_n \sim \mbox{iid } N(\mu,\sigma^2)$

\vspace{1em}
	\begin{block}{Test $H_0\colon \mu = \mu_0$ vs.\ $H_1\colon \mu \neq \mu_0$ at significance level $\alpha$} 
		\begin{itemize}
			\item Test Statistic:  $T_n = \sqrt{n}(\bar{X}_n - \mu_0)/S \sim t(n-1)$ under $H_0$ 
			\item Decision Rule: Reject $H_0$ if $|T_n| > \texttt{qt}(1-\alpha/2, \texttt{df}=n-1)$ 
			\end{itemize}

			\pause
\end{block}
	\begin{block}{$100\times (1-\alpha)\%$ CI for $\mu$} 
		$$\bar{X}_n \pm \texttt{qt}(1-\alpha/2, \texttt{df}=n-1) \frac{S}{\sqrt{n}}$$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Relationship between CI and Two-sided Test}
$\alert{c =  \texttt{qt}(1-\alpha/2, \texttt{df}=n-1)}$
\begin{block}{Decision Rule: Reject $H_0$ if}
		$$\left|\frac{\bar{X}_n - \mu_0}{S/\sqrt{n}} \right|> c \quad \iff \pause  \quad \left(\frac{\bar{X}_n - \mu_0}{S/\sqrt{n}}> c \;\;\mbox{  \alert{OR}  }\;\;\frac{\bar{X}_n - \mu_0}{S/\sqrt{n}}< - c\right)$$
\end{block}

\pause
\begin{block}{Equivalent to: \emph{Don't Reject} $H_0$ provided}
	$$-c \leq \frac{\bar{X}_n - \mu_0}{S/\sqrt{n}}\leq c $$ \pause
	$$\alert{\bar{X_n} - c\times \frac{S}{\sqrt{n}} \leq \mu_0 \leq \bar{X_n} + c\times \frac{S}{\sqrt{n}}}$$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What does this mean?}

\begin{block}
	{Two-sided Test $\iff$ Checking if $\mu_0 \in$ CI}
	A two-sided test of $H_0\colon \mu = \mu_0$ against $H_1\colon \mu\neq \mu_0 $ at significance level $\alpha$ is equivalent to checking whether $\mu_0$ lies inside the corresponding $100\times (1-\alpha)\%$ confidence interval for $\mu$.
\end{block}

\pause

\begin{block}
	{``Inverting'' Two-sided Test to get a CI}	
	Collect all the values $\mu_0$ such that we cannot reject $H_0\colon \mu = \mu_0$ against the two-sided alternative. The result is \emph{precisely} a $100\times (1-\alpha)\%$ CI for $\mu$.
\end{block}
\end{frame}

\section{The Anchoring Effect}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{The Anchoring Experiment}
Shown a ``random'' number and then asked what proportion of UN member states are located in Africa.
	\begin{block}{``Hi'' Group -- Shown 65 ($n_{Hi}=46$)}
		Sample Mean: $30.7$, Sample Variance: $253$
\end{block}


	\begin{block}{``Lo'' Group -- Shown 10 ($n_{Lo}=43$)}
	Sample Mean: $17.1$, Sample Variance: $86$
\end{block}


\vspace{1em}

\hfill\alert{\fbox{Fairly large samples here, so we'll proceed via the CLT...}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{The Anchoring Experiment}
Work together to formulate a null hypothesis, test statistic, and run your test!
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{In words, what is our null hypothesis?\hfill }

	\begin{enumerate}[(a)]
		\item There is a \emph{positive} anchoring effect: seeing a higher random number makes people report a higher answer.
		\item There is a \emph{negative} anchoring effect: seeing a lower random number makes people report a lower answer. 
		\item There \emph{is} an anchoring effect: it could be positive or negative.
		\item There is  \emph{no} anchoring effect: people aren't influenced by seeing a random number before answering.
	\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{In symbols, what is our null hypothesis?}

	\begin{enumerate}[(a)]
		\item $\mu_{Lo} < \mu_{Hi}$
		\item $\mu_{Lo} = \mu_{Hi}$
		\item $\mu_{Lo} > \mu_{Hi}$
		\item $\mu_{Lo} \neq \mu_{Hi}$
	\end{enumerate}
\pause
	\vspace{1em}

	\alert{$\mu_{Lo} = \mu_{Hi}$ is \emph{equivalent to} $\mu_{Hi} - \mu_{Lo} = 0$!}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Anchoring Experiment}
 
Under the null, what should we expect to be true about the values taken on by $\bar{X}_{Lo}$ and $\bar{X}_{Hi}$?

\vspace{1em}

	\begin{enumerate}[(a)]
		\item They should be similar in value.
		\item $\bar{X}_{Lo}$ should be the smaller of the two.
		\item $\bar{X}_{Hi}$ should be the smaller of the two.
		\item They should be different. We don't know which will be larger.
	\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{What is our Test Statistic?}
\begin{block}{Sampling Distribution}
		$$\frac{\left(\bar{X}_{Hi} - \bar{X}_{Lo}\right) - \left(\mu_{Hi} - \mu_{Lo}\right)}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx N(0,1)$$
\end{block}

\begin{block}{Test Statistic: Impose the Null}
Under $H_0\colon \mu_{Lo} = \mu_{Hi}$
	$$T_n =\frac{\bar{X}_{Hi} - \bar{X}_{Lo}}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx N(0,1)$$
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is our Test Statistic?}
\footnotesize
$\bar{X}_{Hi} = 30.7$, $s^2_{Hi} = 253$, $n_{Hi} = 46$\\
$\bar{X}_{Lo} = 17.1$, $s^2_{Lo} = 86$, $n_{Lo} = 43$\\
\normalsize
\vspace{2em}

\begin{block}{Under $H_0\colon \mu_{Lo} = \mu_{Hi}$}
	$$T_n = \frac{\bar{X}_{Hi} - \bar{X}_{Lo}}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx N(0,1)$$
\end{block}

\begin{block}{Plugging in Our Data}
$$T_n = \frac{\bar{X}_{Hi} - \bar{X}_{Lo}}{\sqrt{\frac{S_{Hi}^2}{n_{Hi}} + \frac{S_{Lo}^2}{n_{Lo}}}} \approx 5$$
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
\frametitle{Anchoring Experiment Example }
Approximately what critical value should we use to test $H_0\colon \mu_{Lo} = \mu_{Hi}$ against the two-sided alternative at the 5\% significance level?

\pause
\vspace{1em}
\begin{center}
\begin{tabular}{l|lll}
$\alpha$ &   0.10& 0.05 &0.01\\
\hline
\texttt{qnorm($1-\alpha$)} & 1.28 &1.64 &2.33\\
\texttt{qnorm($1-\alpha/2$)} &1.64 &\alert{1.96}& 2.58
\end{tabular}
\end{center}
\hfill \alert{... Approximately 2}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Anchoring Experiment Example}
Which of these commands would give us the p-value of our test of $H_0\colon \mu_{Lo} = \mu_{Hi}$ against $H_1\colon \mu_{Lo}<\mu_{Hi}$ at significance level $\alpha$?
\vspace{1em}
	\begin{enumerate}[(a)]
		\item \texttt{qnorm($1-\alpha$)}
		\item \texttt{qnorm($1-\alpha/2$)}
		\item \texttt{1 - pnorm(5)}
		\item \texttt{2 * (1 - pnorm(5))}
	\end{enumerate}
	

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{P-values for $H_0\colon \mu_{Lo} = \mu_{Hi}$}
We plug in the value of the test statistic that we observed: 5
\begin{block}{Against $H_1\colon \mu_{Lo}< \mu_{Hi}$}
\texttt{1 - pnorm(5)} $< 0.0000$
\end{block}

\begin{block}{Against $H_1\colon \mu_{Lo}\neq \mu_{Hi}$}
\texttt{2 * (1 - pnorm(5))} $< 0.0000$
\end{block}

\vspace{1em}

\alert{If the null is true (the two population means are equal) it would be extremely unlikely to observe a test statistic as large as this!}

\vspace{1em} 
\hfill \fbox{What should we conclude?}
\end{frame}

\section{Exam Difficulty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Which Exam is Harder?}
% latex.default(round(grades.out, 1), file = "grades.tex", rowname = NULL) 
%
\begin{table}[!tbp]
\begin{center}
\begin{tabular}{rccr}
\hline\hline
\multicolumn{1}{r}{Student}&\multicolumn{1}{c}{Exam 1}&\multicolumn{1}{c}{Exam 2}&\multicolumn{1}{r}{Difference}\tabularnewline
\hline
$ 1$&$57.1$&$60.7$&$  3.6$\tabularnewline
\vdots&\vdots&\vdots&\vdots\\
$71$&$78.6$&$82.9$&$  4.3$\tabularnewline
\hline
Sample Mean: & 79.6 & 81.4  &1.8\\
Sample Var. &117  & 151 & 124\\
Sample Corr.& \multicolumn{2}{c}{0.54}&\\
\hline
\end{tabular}
\end{center}
\end{table}

\vspace{1em}

\fbox{Again, large sample size here so we'll use CLT.}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Exam Difficulty}
Work together to formulate a null hypothesis, test statistic, and run your test!
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{One-Sample Hypothesis Test Using Differences}
\small
\fbox{Let $D_i = X_i - Y_i$ be (Midterm 2 Score - Midterm 1 Score) for student $i$}
\vspace{0.1em}
\begin{block}{Null Hypothesis}
$H_0\colon \mu_1 = \mu_2$, i.e.\ both exams were of the same difficulty
\end{block}
\begin{block}{Two-Sided Alternative}
$H_1\colon \mu_1 \neq \mu_2$, i.e.\ one exam was harder than the other
\end{block}
\begin{block}{One-Sided Alternative}
$H_1\colon \mu_2 > \mu_1$, i.e.\ the second exam was easier
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Decision Rules}
\small
\fbox{Let $D_i = X_i - Y_i$ be (Midterm 2 Score - Midterm 1 Score) for student $i$}
\vspace{0.1em}


\begin{block}
	{Test Statistic}
$$\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}=\frac{1.8}{\sqrt{124/71}} \approx 1.36$$
\end{block}


\begin{block}{Two-Sided Alternative} 
Reject $H_0\colon \mu_1 = \mu_2$ in favor of $H_1\colon \mu_1 \neq \mu_2$ if $|\bar{D}_n|$ is sufficiently large.
\end{block}
\begin{block}{One-Sided Alternative}
Reject $H_0\colon \mu_1 = \mu_2$ in favor of $H_1\colon \mu_2 >\mu_1$ if $\bar{D}_n$ is sufficiently large.
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Reject against \emph{Two-sided} Alternative with $\alpha = 0.1$?  }

	$$\boxed{\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}= \frac{1.8}{\sqrt{124/71}} \approx 1.36} $$

\begin{center}
\begin{tabular}{l|lll}
$\alpha$ &   0.10& 0.05 &0.01\\
\hline
\texttt{qnorm($1-\alpha$)} & 1.28 &1.64 &2.33\\
\texttt{qnorm($1-\alpha/2$)} &1.64 &1.96& 2.58
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Reject
\item Fail to Reject
\item Not Sure
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Reject against \emph{One-sided} Alternative with $\alpha = 0.1$?  }

	$$\boxed{\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}= \frac{1.8}{\sqrt{124/71}} \approx 1.36} $$

\begin{center}
\begin{tabular}{l|lll}
$\alpha$ &   0.10& 0.05 &0.01\\
\hline
\texttt{qnorm($1-\alpha$)} & 1.28 &1.64 &2.33\\
\texttt{qnorm($1-\alpha/2$)} &1.64 &1.96& 2.58
\end{tabular}
\end{center}

\begin{enumerate}[(a)]
\item Reject
\item Fail to Reject
\item Not Sure
\end{enumerate}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{P-Values for the Test of $H_0\colon \mu_1 = \mu_2$}

	$$\boxed{\displaystyle \frac{\bar{D}_n}{\widehat{SE}(\bar{D}_n)}= \frac{1.8}{\sqrt{124/71}} \approx 1.36} $$

\begin{block}{One-Sided $H_1\colon \mu_2 > \mu_1 $} 
$\texttt{1 - pnorm(1.36)} =  \texttt{pnorm(-1.36)}  \approx 0.09$ 
\end{block}

\begin{block}{Two-Sided $H_1 \colon \mu_1 \neq \mu_2$} 
$\texttt{2 * (1 - pnorm(1.36))} =  \texttt{2 * pnorm(-1.36)} \approx 0.18$
\end{block}
\end{frame}

\section{2012 Voter Polls}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Tests for Proportions}
	\begin{block}
		{Basic Idea}
		The population \emph{can't be} normal (it's Bernoulli) so we use the CLT to get approximate sampling distributions.
	\end{block}
	\begin{block}
		{But there's a small twist!}
		Bernoulli RV only has a \emph{single} unknown parameter $\implies$  we know \emph{more} about the population under $H_0$ in a proportions problem than in the other testing examples we've examined...	
	\end{block}

\vspace{1em}	

	\hfill\alert{\fbox{For best results, always \emph{fully} impose the null.}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Tests for Proportions: One-Sample Example}
	\begin{block}
		{From Pew Polling Data}
		54\% of a random sample of 771 registered voters correctly identified 2012 presidential candidate Mitt Romney as Pro-Life.
	\end{block}
	\begin{block}
		{Sampling Model}
		$X_1, \hdots, X_{n} \sim \mbox{iid Bernoulli}(p)$
	\end{block}
	\begin{block}
		{Sample Statistic}
		Sample Proportion: $\displaystyle\widehat{p} = \frac{1}{n}\sum_{i=1}^{n} X_i$
	\end{block}

	\vspace{1em}

	\hfill \alert{\fbox{Suppose I wanted to test $H_0\colon p = 0.5$}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Tests for Proportions: One Sample Example}
	Under $H_0\colon p = 0.5$ what is the standard error of $\widehat{p}$?

	\begin{enumerate}[(a)]
		\item $1$
		\item $\sqrt{\widehat{p}(1-\widehat{p})/n}$
		\item $\sigma/\sqrt{n}$
		\item $1/(2\sqrt{n})$
		\item $p(1-p)$ 
	\end{enumerate}
\pause
 \alert{$p=0.5 \implies \sqrt{0.5(1-0.5)/n} = 1/(2\sqrt{n})$}

 \vspace{1em}
 \emph{Under the null we know the SE! Don't have to estimate it!}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{One-Sample Test for a Population Proportion}
	\begin{block}
		{Sampling Model}
		$X_1, \hdots, X_n \sim \mbox{iid Bernoulli}(p)$
	\end{block}
	\begin{block}
		{Null Hypothesis}
		$H_0 \colon p = \mbox{Known Constant } p_0$
	\end{block}
	\begin{block}
		{Test Statistic} 
		$\displaystyle T_n = \frac{\widehat{p} - p_0}{\sqrt{p_0(1-p_0)/n}} \approx N(0,1)$ under $H_0$ provided $n$ is large
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{One-Sample Example $H_0\colon p = 0.5$}
	\fbox{\footnotesize 54\% of a random sample of 771 registered voters knew Mitt Romney is Pro-Life.}

\vspace{1em}

	\begin{eqnarray*}
		T_n &=& \frac{\widehat{p} - p_0}{\sqrt{\displaystyle \frac{p_0(1 - p_0)}{n}}} = 2 \sqrt{771}(0.54 - 0.5)\\ \\
		&=& 0.08 \times \sqrt{771} \approx 2.2
	\end{eqnarray*}
	\begin{block}
		{One-Sided p-value}
		\texttt{1 - pnorm(2.2)} $\approx 0.014$
	\end{block}
	\begin{block}
		{Two-Sided p-value}
		\texttt{2 * (1 - pnorm(2.2))} $\approx 0.028$
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Tests for Proportions: Two-Sample Example}
	\begin{block}
		{From Pew Polling Data}
		53\% of a random sample of 238 Democrats correctly identified Mitt Romney as Pro-Life versus 61\% of 239 Republicans.
	\end{block}
	\begin{block}
		{Sampling Model}
		Republicans: $X_1, \hdots, X_{n} \sim \mbox{iid Bernoulli}(p)$ independent of\\
		Democrats: $Y_1, \hdots,Y_{m} \sim \mbox{iid Bernoulli}(q)$ 
	\end{block}
	\begin{block}
		{Sample Statistics}
		Sample Proportions: $\displaystyle\widehat{p} = \frac{1}{n}\sum_{i=1}^{n} X_i, \quad\displaystyle\widehat{q} = \frac{1}{m}\sum_{i=1}^{m} Y_i$
	\end{block}

	\vspace{1em}

	\hfill \alert{\fbox{Suppose I wanted to test $H_0\colon p = q$}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A More Efficient Estimator of the SE Under $H_0$}
	\begin{alertblock}
		{Don't Forget!}
		Standard Error (SE) means ``std.\ dev.\ of sampling distribution'' so you should know how to prove that that:
	$$SE(\widehat{p} - \widehat{q}) = \sqrt{\frac{p(1-p)}{n} + \frac{q(1-q)}{m}}$$
	\end{alertblock}

	\begin{block}
		{Under $H_0\colon p = q$}
		\emph{Don't} know values of $p$ and $q$: only that they are equal.
	\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{A More Efficient Estimator of the SE Under $H_0$}
	\begin{block}
		{One Possible Estimate}
	$$\widehat{SE} = \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n} + \frac{\widehat{q}(1-\widehat{q})}{m}}$$
	\end{block}
	\begin{block}
		{A \emph{Better} Estimate Under $H_0$}
		$$\widehat{SE}_{Pooled} = \sqrt{\widehat{\pi}(1-\widehat{\pi})\left( \frac{1}{n} + \frac{1}{m} \right) } \quad \mbox{where}\quad \widehat{\pi} = \displaystyle \frac{n \widehat{p} + m \widehat{q}}{n + m}$$
	\end{block}
	\begin{alertblock}
		{Why Pool?}
		If $p = q$, the two populations \emph{are the same}. This means we can get a \emph{more precise} estimate of the \emph{common} population proportion by pooling. More data = Lower Variance $\implies$ better estimated SE.
	\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Two-Sample Test for Proportions}
	\begin{block}
		{Sampling Model}
		\small
		$X_1, \hdots, X_n \sim \mbox{iid Bernoulli}(p)$ indep.\  of $Y_1, \hdots, Y_m \sim \mbox{iid Bernoulli}(q)$
	\end{block}
	\begin{block}
		{Sample Statistics}
			Sample Proportions: $\displaystyle\widehat{p} = \frac{1}{n}\sum_{i=1}^{n} X_i, \quad\displaystyle\widehat{q} = \frac{1}{m}\sum_{i=1}^{m} Y_i$
	\end{block}
	\begin{block}
		{Null Hypothesis}
		$H_0\colon p = q \quad \Leftarrow \; $\fbox{ i.e.\ $p - q = 0$}
	\end{block}
	\begin{block}
		{Pooled Estimator of SE under $H_0$}
		$\widehat{\pi} = \displaystyle \frac{n \widehat{p} + m \widehat{q}}{n + m}, \quad \widehat{SE}_{Pooled} = \sqrt{\widehat{\pi}(1-\widehat{\pi})\left( 1/n + 1/m \right) }$
	\end{block}
	\begin{block}
		{Test Statistic}
		$\displaystyle T_n = \frac{\widehat{p}- \widehat{q}}{\widehat{SE}_{Pooled}} \approx N(0,1)$ under $H_0$ provided $n$ and $m$ are large
	\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Two-Sample Example $H_0\colon p = q$}
	\fbox{\footnotesize 53\% of 238 Democrats knew Romney is Pro-Life vs.\ 61\% of 239 Republicans}
	\small
$$\widehat{\pi} = \frac{n\widehat{p}+ m\widehat{q}}{n + m} = \frac{239 \times 0.61 + 238 \times 0.53}{239 + 238}\approx 0.57$$
	\begin{eqnarray*}
	\widehat{SE}_{Pooled} &=&  \sqrt{\widehat{\pi}(1-\widehat{\pi})\left( 1/n + 1/m \right) }= \sqrt{0.57 \times 0.43 (1/239 + 1/238)}\\
		&\approx& 0.045
	\end{eqnarray*}
$$T_n = \frac{\widehat{p} - \widehat{q}}{\widehat{SE}_{Pooled}}= \frac{0.61 - 0.53}{0.045} \approx 1.78$$
\begin{block}
	{One-Sided P-Value}
	\texttt{1 - pnorm(1.78)}$\approx 0.04$
\end{block}\begin{block}
	{Two-Sided P-Value}
	\texttt{2 * (1 - pnorm(1.78))}$\approx 0.08$
\end{block}
\end{frame}

\section{Biased Scales}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{block}
	{Experiment}
	\begin{itemize}
		\item Weigh a known 10 gram mass 16 times on the same scale.
		\item Scale makes normally distributed measurement errors:
		$X_1, \hdots, X_{16} \sim \mbox{iid } N( \mu, \sigma^2 = 4)$
	\end{itemize}
\end{block}
\begin{alertblock}
			{Measurement Errors?}
		 Weigh same object repeatedly $\Rightarrow$ slightly different result each time. Average deviation from mean $\approx 2$ grams.
		\end{alertblock}
\begin{block}{Two Kinds of Scales}
	\begin{description}
			\item[Unbiased] Correct on average: $\mu = 10$ grams
			\item[Biased] \emph{Too high} on average: $\mu = 11$ grams
		\end{description}	
		
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{An Idea for Deciding if a Scale is Biased}

\begin{enumerate}
	\item Test $H_0 \colon \mu = 10$ against $H_1\colon \mu > 10$ with $\alpha =0.025$.
	\item Decide based on the outcome of test:
		\begin{itemize}
	\item Reject $H_0 \Rightarrow$ decide scale is biased, throw it away.
	\item Fail to reject $H_0 \Rightarrow$ decide scale is unbiased, keep it.
			\end{itemize}	
\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
	\frametitle{Testing Whether a Scale is Biased }
	\fbox{$X_1, \hdots, X_{16} \sim \mbox{iid } N( \mu, \sigma^2)$ where we \emph{know} $\sigma^2 = 4$}

	\vspace{2em}

	Suppose I want to test $H_0\colon \mu = 10$. What is my test statistic? 

	\vspace{1em}

	\begin{enumerate}[(a)]
		\item $4\bar{X}/S$  
		\item $4(\bar{X} - 10)/S$
		\item $(\bar{X} - \mu)/(S/\sqrt{n})$
		\item $2 \bar{X}$
		\item $2(\bar{X} - 10)$ 
	\end{enumerate}

	\pause
	\alert{$$T_n = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} = \frac{\bar{X} - 10}{2/\sqrt{16}} = 2(\bar{X} - 10)$$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]

	\frametitle{Testing Whether a Scale is Biased }
	\fbox{$X_1, \hdots, X_{16} \sim \mbox{iid } N( \mu, \sigma^2)$ where we \emph{know} $\sigma^2 = 4$}

	\vspace{2em}

	What is the sampling distribution of $2(\bar{X}-10)$ under $H_0\colon \mu = 10$? 

	\vspace{1em}

	\begin{enumerate}[(a)]
		\item $N(\mu, 4)$  
		\item $N(0, 4)$  
		\item $t(15)$  
		\item $\chi^2(15)$  
		\item $N(0,1)$  
	\end{enumerate}

	\pause
	\alert{$H_0\colon \mu = 0 \implies \displaystyle T_n = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} =  2(\bar{X} - 10)\sim N(0,1)$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
	\frametitle{Testing Whether a Scale is Biased }
	\fbox{$X_1, \hdots, X_{16} \sim \mbox{iid } N( \mu, \sigma^2)$ where we \emph{know} $\sigma^2 = 4$}

	\vspace{1em}

	Suppose I want to test $H_0\colon \mu = 10$ against the \emph{one-sided} alternative $\mu > 10$ with $\alpha = 0.025$. What is my decision rule? 

	\vspace{1em}

	\begin{enumerate}[(a)]
		\item Reject $H_0$ if $2(\bar{X} - 10) > 1$ 
		\item Reject $H_0$ if $2(\bar{X} - 10) < 1$ 
		\item Reject $H_0$ if $2(\bar{X} - 10) > 2$ 
		\item Reject $H_0$ if $2(\bar{X} - 10) < 2$ 
		\item Reject $H_0$ if $|2(\bar{X} - 10)| > 2$ 
	\end{enumerate}

	\pause
	\alert{Reject $H_0$ if $T_n = 2(\bar{X} - 10) >$ \texttt{qnorm(1 - 0.025)} $\approx 2$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Testing an \emph{Unbiased} Scale}
	 Unbeknowst to me the scale I am testing is in fact \emph{unbiased}.
	 What is the probability that I will decide, based on the outcome of my test, to throw it away?

	 \vspace{2em}
	\pause

	\alert{This is simply a Type I Error! Hence the probability is $\alpha = 0.025$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Testing a \emph{Biased} Scale}
	 Unbeknowst to me the scale I am testing is in fact \emph{biased}.
	 What is the probability that I will decide, based on the outcome of my test, to throw it away?

\pause

\vspace{1em}

\alert{This is the \emph{opposite} of a Type II error...}
\end{frame}

\section{Power}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{What is the probability of throwing away a \em{biased} scale?}

	\begin{block}
		{Decision Rule}
		Decide scale is biased if $2(\bar{X} - 10) > 2$ or \emph{equivalently} if \alert{$\bar{X} > 11$} 
	\end{block}
	\begin{block}
		{Biased Scale}
		$\mu = 11 \quad \implies \quad X_1, \hdots, X_{16} \sim \mbox{iid } N(11, \sigma^2 = 4)$
	\end{block}
	\begin{alertblock}
		{Which implies...}
	\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Testing a \emph{Biased} Scale}

	Suppose $X_1, \hdots, X_{16} \sim N(11, \sigma^2 = 4)$. What is the sampling distribution of $\bar{X}$?
	
	\vspace{1em}

	\begin{enumerate}[(a)]
		\item $N(11, 1)$
		\item $N(0, 1)$ 
		\item $t(15)$
		\item $N(11, 1/4)$ 
		\item $N(10, 1/4)$
	\end{enumerate}
\pause

\vspace{1em}

\alert{$$\bar{X}_n \sim N(\mu, \sigma^2/n) = N(11, 1/4)$$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{What is the probability of throwing away a \em{biased} scale?}

	\begin{block}
		{Decision Rule}
		Decide scale is biased if $2(\bar{X} - 10) > 2$ or \emph{equivalently} if \alert{$\bar{X} > 11$} 
	\end{block}
	\begin{block}
		{Biased Scale}
		$\mu = 11 \quad \implies \quad X_1, \hdots, X_{16} \sim \mbox{iid } N(11, \sigma^2 = 4)$
	\end{block}
	\begin{alertblock}
		{Which implies}
		$\bar{X} \sim N(11, 1/4)\quad \implies \quad$  \alert{$P(\bar{X}>11) = 1/2$} 
	\end{alertblock}

\vspace{1em}

	\alert{\fbox{The \emph{power} of this test is 50\%}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Recall:}

\begin{block}
	{Type I Error} Rejecting $H_0$ when it is true:  $P(\mbox{Type I Error}) = \alpha$
\end{block}

\begin{block}
	{Type II Error} Failing to reject $H_0$ when it is false: \alert{$P(\mbox{Type II Error}) =\beta$}
\end{block}

\begin{alertblock}
	{Statistical Power} The probability of rejecting $H_0$ when it is false: \alert{$\mbox{Power} = 1 -\beta $}\\ i.e.\ the probability of \emph{convicting} a guilty person.
\end{alertblock}

\vspace{1em}

\begin{center}
\alert{\boxed{
\begin{minipage}
	{0.95\textwidth}
	Hypothesis tests designed to control Type I error rate ($\alpha$). But we also care about Type II errors. What can learn about these?
\end{minipage}}}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Recall: Normal Population Known Variance}
	\begin{block}
		{Sampling Model}
		$X_1, \hdots, X_n \sim N(\mu, \sigma^2)$ where $\sigma^2$ is known
	\end{block}
	\begin{block}
		{Sampling Distribution}
		$$\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$$
	\end{block}
	\begin{block}
		{Under $H_0\colon \mu = 0$}
		$$T_n = \frac{\bar{X}_n}{\sigma/\sqrt{n}} \sim N(0,1)$$
	\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{What happens if $\mu \neq 0$?}
	\begin{alertblock}
		{Key Point \#1}
		\begin{itemize}
			\item Test Statistic $T_n=\sqrt{n}(\bar{X}_n/\sigma)$
			\item Unless $\mu = 0$, test statistic is \emph{not} standard normal!
			\item When $\mu \neq 0$, distribution of $T_n$ \emph{depends on} $\mu$!	
		\end{itemize}
	\end{alertblock}
	\begin{alertblock}
		{Key Point \#2}
		\emph{Regardless} of the value of $\mu$,
			$$\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$$
		since the population is normally distributed!
	\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Distribution of $T_n$ \emph{Under the Alternative}}
	\begin{eqnarray*}
		T_n &=& \frac{\bar{X}_n}{\sigma/\sqrt{n}}\\\\ \pause
		&=& \frac{\bar{X}_n}{\sigma/\sqrt{n}} \alert{- \frac{\mu}{\sigma/\sqrt{n}} + \frac{\mu}{\sigma/\sqrt{n}}} \\ \\ \pause
		&=& \left( \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \right) + \frac{\mu}{\sigma/\sqrt{n}} \\ \\ \pause
		&=& Z + \sqrt{n}(\mu/\sigma) \pause 
		\sim \alert{N\left( \sqrt{n}(\mu/\sigma),1  \right)} 
	\end{eqnarray*}
	Where $Z \sim N(0,1)$
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Power of One-Sided Test}
\small
\begin{block}
	{Under the Alternative}
	$T_n = \sqrt{n}(\bar{X}_n/\sigma) \sim N\left( \sqrt{n}(\mu/\sigma),1  \right)$
\end{block}
\begin{block}
	{Decision Rule}
	Reject $H_0\colon \mu = 0$ if $T_n> \texttt{qnorm}(1- \alpha)$  
\end{block}
\begin{eqnarray*}
	1 - \beta &=& P(\mbox{Reject } H_0|H_0 \mbox{ false}) \pause = P(T_n >\texttt{qnorm}(1- \alpha))\\ \pause
	&=& P \left( Z + \sqrt{n}(\mu/\sigma) > \texttt{qnorm}(1- \alpha)\right)\\ \pause
	&=& P \left( Z >    \texttt{qnorm}(1- \alpha)-\sqrt{n}(\mu/\sigma)\right)\\ \pause 
	&=& 1 - P \left( Z \leq  \texttt{qnorm}(1- \alpha)-\sqrt{n}(\mu/\sigma)\right) \\
	&=& 1 - \texttt{pnorm}\left( \texttt{qnorm}(1 - \alpha) - \sqrt{n}(\mu/\sigma) \right) 
\end{eqnarray*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{\href{https://fditraglia.shinyapps.io/power_oneside}{https://fditraglia.shinyapps.io/power\_oneside}}

\begin{figure}
	\fbox{\includegraphics[scale = 0.3]{./images/power_oneside_screenshot}}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Power of Two-Sided Test}
\footnotesize
\begin{block}
	{Under the Alternative}
	$T_n = \sqrt{n}(\bar{X}_n/\sigma) \sim N\left( \sqrt{n}(\mu/\sigma),1  \right)$
\end{block}
\begin{block}
	{Decision Rule}
	Reject $H_0\colon \mu = 0$ if $|T_n|> \texttt{qnorm}(1- \alpha)$  
\end{block}
\begin{eqnarray*}
	1 - \beta &=& P(\mbox{Reject } H_0|H_0 \mbox{ false}) \pause = P(|T_n| >\texttt{qnorm}(1- \alpha/2))\\ \pause
		&=&\underbrace{P(T_n < -\texttt{qnorm}(1- \alpha/2)}_{\text{\alert{Lower}}} + \underbrace{P(T_n >\texttt{qnorm}(1- \alpha/2)}_{\text{\alert{Upper}}}\\ \\ \pause
	\mbox{\alert{Upper}} &=& (\mbox{Power of One-Sided Test with } \alpha/2 \mbox{ instead of } \alpha)\\ \pause
		&=& 1 - \texttt{pnorm}\left( \texttt{qnorm}(1 - \alpha/2) - \sqrt{n}(\mu/\sigma) \right) \\ \\ \pause
	\mbox{\alert{Lower}} &=& \texttt{pnorm}\left(-\texttt{qnorm}(1 - \alpha/2) - \sqrt{n}(\mu/\sigma) \right) \\
\end{eqnarray*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{\href{https://fditraglia.shinyapps.io/power_twoside}{https://fditraglia.shinyapps.io/power\_twoside}}

\begin{figure}
	\fbox{\includegraphics[scale = 0.3]{./images/power_twoside_screenshot}}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What Determines Power?}
	\begin{block}{Power $ = 1 -  P$(Type II Error)}
Chance of detecting an effect given that one exists.
\end{block}
\begin{block}{Depends On:}
	\begin{enumerate}
\item Magnitude of Effect: \emph{true} value of $\mu$
	\begin{itemize}
		\item Easier to detect large deviations from $H_0\colon \mu = 0$
	\end{itemize}
\item Amount of variability in the population: $\sigma$
	\begin{itemize}
		\item Lower $\sigma \Rightarrow$ easier to detect effect of given magnitude
	\end{itemize}
\item Sample Size: $n$
\begin{itemize}
	\item Larger sample size $\Rightarrow$ easier to detect effect of given magnitude 
\end{itemize}
\item Significance Level: $\alpha$
\begin{itemize}
	\item Fewer Type I errors $\Rightarrow$ more Type II errors
\end{itemize}
\end{enumerate}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Study Tip}
		Compare determinants of \emph{width} of $(1- \alpha)\times100\%$ CI to determinants of \emph{power} of corresponding two-sided test.
\end{frame}

\section{Final Thoughts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Terminology I Have Intentionally Avoided Until Now}

\begin{block}{Statistical Significance}
Suppose we carry out a hypothesis test at the $\alpha\%$ level and,  based on our data, reject the null. You will often see this situation described as ``statistical significance.''
\end{block}

\begin{block}{In Other Words...}
When people say ``statistically significant'' what they really mean is that they rejected the null hypothesis.
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Some Examples}

	\begin{itemize}
		\item We found a difference between the ``Hi'' and ``Lo'' groups in the anchoring experiment that was statistically significant at the 5\% level based on data from a past semester.
		\item Our 95\% CI for the proportion of US voters who know who John Roberts is did not include 0.5. Viewed as a two-sided test, we found that the difference between the true population proportion and 0.5 was statistically significant at the 5\% level.
	\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Why Did I Avoid this Terminology?}
\small
\begin{block}{Statistical Significance $\neq$ Practical Importance}
	\begin{itemize}
		\item You need to understand the term ``statistically significant'' since it is widely used. A better term for the idea, however, would be ``statistically discernible''
		\item Unfortunately, many people are confuse ``significance'' in the narrow, technical sense with the everyday English word meaning ``important'' 
		\item \alert{Statistically Significant Does Not Mean Important!"}
			\begin{itemize}
				\item A difference can be practically unimportant but statistically significant.
				\item A difference can be practically important but statistically insignificant.
			\end{itemize}
	\end{itemize}
\end{block}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\begin{center}
\Huge P-value Measures Strength of Evidence Against $H_0$\\ \alert{Not The Size of an Effect!}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Statistically Significant but Not Practically Important}
\small
I flipped a coin 10 million times (in R) and got 4990615 heads.
\begin{block}{Test of $H_0\colon p = 0.5$ against $H_1\colon p \neq 0.5$}
$$T = \displaystyle \frac{\widehat{p} - 0.5}{\sqrt{0.5(1-0.5)/n}} \approx -5.9   \implies \alert{\mbox{ p-value } \approx 0.000000003}$$
\end{block}

\begin{block}{Approximate 95\% Confidence Interval}
 $$\widehat{p} \pm \texttt{qnorm}(1 - 0.05/2) \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}  \implies \alert{(0.4988, 0.4994)}$$
\end{block}

\footnotesize (Such a huge sample size that refined vs.\ textbook CI makes no difference)
\large
\vspace{1em}

\alert{\fbox{Actual $p$ was 0.499}}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Practically Important But Not Statistically Significant}
\framesubtitle{\href{http://www.amazon.com/p-value-Stories-Actually-Understand-Statistics/dp/0321629302}{\fbox{Vickers: ``What is a P-value Anyway?'' (p. 62)}}}
\footnotesize
\begin{quote}
Just before I started writing this book, a study was published reporting about a 10\% lower rate of breast cancer in women who were advised to eat less fat. If this indeed the true difference, low fat diets could reduce the incidence of breast cancer by tens of thousands of women each year -- astonishing health benefit for something as simple and inexpensive as cutting down on fatty foods. The p-value for the difference in cancer rates was 0.07 and here is the key point: this was widely misinterpreted as indicating that low fat diets don't work. For example, the \emph{New York Times} editorial page trumpeted that ``low fat diets flub a test'' and claimed that the study provided ``strong evidence that the war against all fats was mostly in vain.'' \alert{However failure to prove that a treatment is effective is not the same as proving it ineffective.}
\end{quote}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{Do Students with 4-Letter Surnames Do Better?}
 \framesubtitle{Based on Data from Midterm 1 Last Semester}
    \begin{columns}
    	\column{0.35\textwidth} \begin{block}
    		{4-Letter Surname}
    			$\bar{x} = 88.9$\\
    			$s_x = 10.4$\\
    			$n_x = 12$
    	\end{block} 
    	\column{0.35\textwidth} \begin{block}
    		{Other Surnames}
    			$\bar{y} = 74.4$\\
    			$s_y = 20.7$\\
    			$n_y = 92$
    	\end{block}
    \end{columns}

\vspace{1em}
\begin{block}
	{Difference of Means}
	$\bar{x} - \bar{y} = \alert{14.5}$
\end{block}
\begin{block}
	{Standard Error}
	$\displaystyle SE = \sqrt{s_x^2/n_x + s_y^2/n_y} \approx \alert{3.7}$
\end{block}
\begin{block}
	{Test Statistic}
	$T = 14.5 / 3.7 \approx \alert{3.9}$
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{What is the p-value for the two-sided test? }
    
$$\boxed{\mbox{Test Statistic} \approx 3.9}$$

\begin{enumerate}[(a)]
	\item $p < 0.01$
	\item $0.01 \leq p < 0.05$
	\item $0.05 \leq p < 0.1$
	\item $p > 0.1$
	\item Not Sure
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c]\frametitle{What do these results mean? }

Evaulate this statement in light of our hypothesis test:
\vspace{1em}

\begin{quote}
	Students with four-letter long surnames do better, on average, on the first midterm of Econ 103 at UPenn.
\end{quote}

\begin{enumerate}[(a)]
	\item Strong evidence in favor
	\item Moderate evidence in favor
	\item No evidence either way
	\item Moderate evidence against
	\item Strong evidence against
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[c,fragile]\frametitle{I just did 134 Hypothesis Tests...}
   
 \begin{block}
 	{... and 11 of them were significant at the 5\% level.}
 \end{block}

\footnotesize

\begin{verbatim}
         group sign p.value x.bar N.x  s.x y.bar N.y  s.y
26  first1 = P    1   0.000  93.8   3  2.9  75.5 101 20.4
70     id2 = 7    1   0.000  94.6   5  3.3  75.1  99 20.4
134    id8 = 0    1   0.000  92.6   7  4.9  74.8  97 20.5
5    Nlast = 4    1   0.001  88.9  12 10.4  74.4  92 20.7
90     id4 = 8    1   0.003  87.7   9  9.0  74.9  95 20.7
105    id6 = 8    1   0.003  88.1   5  5.8  75.4  99 20.6
109    id6 = 2    1   0.007  88.9   8 10.7  75.0  96 20.6
9    Nlast = 2    1   0.016  90.4   5  9.3  75.3  99 20.5
49   last1 = P   -1   0.036  65.2   6  9.9  76.7  98 20.6
65     id2 = 1    1   0.038  84.3   9 10.1  75.3  95 20.9
117    id7 = 8    1   0.041  83.4  13 11.6  75.0  91 21.1
\end{verbatim}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Data-Dredging}
\begin{itemize}
	\item Suppose you have a long list of null hypotheses and assume, for the sake of argument that all of them are true.
		\begin{itemize}
			\item E.g.\ there's no difference in grades between students with different 4th digits of their student id number. 
		\end{itemize}
	\item We'll still reject about 5\% of the null hypotheses.
	\item Academic journals tend only to publish results in which a null hypothesis is rejected at the 5\% level or lower. 
	\item We end up with the bizarre result that ``most published studies are false.''  
\end{itemize}


\alert{I posted a reading about this on Piazza: ``The Economist - Trouble in the Lab.'' To learn even more, see \href{http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124}{\textcolor{blue}{\fbox{Ioannidis (2005)}}}}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Green Jelly Beans Cause Acne!}
\framesubtitle{\href{http://xkcd.com/882/}{\fbox{xkcd \#882}}}
\begin{figure}
\centering
	\includegraphics[scale=0.45]{./images/xkcd1}
	\caption{Go and read this comic strip: before today's lecture you wouldn't have gotten the joke!}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
% \frametitle{Don't Compare P-Values Across Different Tests!}
% \framesubtitle{\fbox{\href{http://www.people-press.org/files/2012/08/8-10-12-Knowledge-release.pdf}{Pew: ``What Voters Know About Campaign 2012''}}}


% \footnotesize

% Of 239 Republicans, 61\% knew Romney is pro-life vs.\ 53\% of 238 Democrats.
% \pause
% \begin{block}{$H_0\colon p_{Rep} = 0.5$ vs.\ $H_1\colon p_{Rep} \neq 0.5$}
%  $$T = \frac{0.61 - 0.5}{\sqrt{0.5(1-0.5)/239}} \approx  3.4 \implies \mbox{ p-value } \approx 0.0007$$
% \end{block}
% \pause
% \begin{block}{$H_0\colon q_{Dem} = 0.5$ vs.\ $H_1\colon q_{Dem} \neq 0.5$}
%  $$T = \frac{0.53 - 0.5}{\sqrt{0.5(1-0.5)/238}} \approx 0.93  \implies \mbox{ p-value } \approx 0.35$$
% \end{block}
% \pause
% \begin{block}{$H_0\colon p_{Rep} =q_{Dem}$ vs.\ $H_1\colon p_{Rep} \neq q_{Dem}$}
%  $$T = \frac{0.61 - 0.53}{\sqrt{\left(\frac{1}{239}+ \frac{1}{238}\right)\left(\frac{239 \times 0.61 + 238 \times 0.53}{239 + 238}\right)}} \approx  1.76 \implies \mbox{ p-value } \approx 0.08$$
% \end{block}


% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}
% \frametitle{Don't Compare P-Values Across Different Tests!}

% \begin{itemize}
% 	\item P-Value measures strength of evidence against the null, not the size of an affect! \pause
% 	\item Use a single test to address a single research question: if you are actually interested in differences between Republicans and Democrats, test for this directly! \pause
% \end{itemize}

% \vspace{1em}

% \pause

% For more on the problems associated with comparing p-values from different hypothesis tests, along with an even starker example than the one I just showed you, see \href{http://amstat.tandfonline.com/doi/abs/10.1198/000313006X152649}{\textcolor{blue}{\fbox{Gelman \& Stern (2006)}}}

% \end{frame}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Some Final Thoughts}
	\begin{itemize}
		\item Failing to reject $H_0$ does not mean $H_0$ is true. 
		\item Rejecting $H_0$ does not mean $H_1$ is true.
		\item P-values are always more informative than simply reporting ``Reject'' vs.\ ``Fail To Reject'' at a given significance level. 
		\item Confidence intervals are more informative that hypothesis tests, since they give an idea of the size of an effect. 
		\item If $H_0$ is actually plausible a priori (this is rarer than you may think), reporting a p-value can be a good complement to a CI. 
		\item To avoid data-dredging be honest about the tests you have carried out: report \emph{all of them}, not just the ones where you rejected the null.
	\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}